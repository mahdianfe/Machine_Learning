#---
---
"""
نکته برداری از سایت های:

1. https://faradars.org/courses/machine-learning-using-python-fvpht0091

 2. و چت جی پی تی و جیمینی

 """



# !pip uninstall scikit-learn
# !pip install scikit-learn==1.2.0
# %%
import sklearn

sklearn.__version__
# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %% md
# Reading data
# %%
df = pd.read_csv("housing.csv")
# %%
df
# %%
df.info()
# %%
# تعداد برای متغیرهایی که در خانه های مجاور اقیانوس امده
df.ocean_proximity.value_counts()
# %%
type(df)
# %%
df.describe()
# %%

# %% md

# %% md
 #_________________________________________________________________________________________________________________________________________
# Train and Test set 1
#### این نوع جدا کردن منظور ما نیست

# %%
# def shuffle_and_split_df(df, test_ratio):
#   np.random.seed(40)
#   random_indices = np.random.permutation(len(df))
#   test_set_size = int(len(df) * test_ratio)
#   test_random_indices = random_indices[:test_set_size]
#   train_random_indices = random_indices[test_set_size:]
#   return df.iloc[train_random_indices], df.iloc[test_random_indices]
# %%
# train_set, test_set = shuffle_and_split_df(df, 0.2)
# %%
# test_set
# %%
# from zlib import crc32
# %%
# crc32(np.int64(10))
# %%
# def is_identifier_in_test_set(identifier, test_ratio):
#   return crc32(np.int64(identifier)) < test_ratio*2**32
# %%
# def split_train_test_with_identifier_hash(df, test_ratio, identifier_column):
#   identifiers = df[identifier_column]
#   in_test_set = identifiers.apply(lambda id_: is_identifier_in_test_set(id_, test_ratio))
#   return df.loc[~in_test_set], df.loc[in_test_set]
# %%
# train_set, test_set = split_train_test_with_identifier_hash(df.reset_index(), 0.2, "index")
# %%
# df_with_identifier = df
# df_with_identifier["identifier"] = df["longitude"]*1000 + df["latitude"]
# df_with_identifier
# %%
# train_set, test_set = split_train_test_with_identifier_hash(df_with_identifier,
#                                                             0.2,
#                                                             "identifier")
# %%
# train_set
# %%
# test_set
# %% md
 #_________________________________________________________________________________________________________________________________________
# Train and Test set
#### این روش مد نظر ماست

# %%
from sklearn.model_selection import train_test_split

# %%
train_set, test_set = train_test_split(df, test_size=0.2, random_state=40)
# %%
train_set
# %%
test_set
# %%
# پیدا کردن مقدار Nan در X
print(train_set.isnull().sum())
# %% md
 #_________________________________________________________________________________________________________________________________________
# نمایش اطلاعات دیتا فریم
# %%

# خود پانداس مت پلات لیب را فراخانی میکنه
df.hist(bins=40, figsize=(20, 9))
# %%
df["median_house_value"].value_counts()
# %% md
"""
# نکته مهم
# ستون های تک که اخر بعضی نمودارها مثل مدین هوس ولیو وجود دارد
     این عجیبه
     یعنی روی مثلا 500 هزار دلار یه پیک میبینیم 
     در اصل اومده مقادیر که از یک جایی به بعد کم میشده را قطع کردیم 
     و همه اعداد بزرگتر از 500 هزار دلار را را روی 500هزار دلار گذاشته و جمع کرده
     اما این مقدار اثر میگذاره در حالیکه این تارگت ما است.
    نمیتونیم مدلی بنویسیم که برای خانه های بیشتر از 500 هزار دلار نمیتونه پیش بینی انجام بده
    بنابراین باید 500هزار دلار را حذف کنیم تا در پیش بینی مدل اثر نگذاره
"""

# %% md
 #_________________________________________________________________________________________________________________________________________
# جلوگیری از سمپلینگ بایاس یا سوگیری نمونه گیری
"""
مثلا دو نمونه اقا و خانوم داریم که درصد وجود اقا و خانم 50 50 نیست و یکی 80 است خب این نمونه نتیجه درستی را به ما نمیده
بنابراین می اییم برای اسپلیت کردن تست و ترین از هم میگوییم که 20 درصد از 80 درصد خانم را بردار و 20 درصد از 20 درصد اقا را بردار که سوگیری اتفاق نیفته 
"""

#### راه حل: طبقه بندی نمونه


#### روش:
####  الف. روی نمونه خودمان طبقه بندی انجام دهیم
#### و ب. سپس تست و ترین را مجدد این بار برای کتگوری ای که ایجاد کردیم از هم جدا میکنیم
#### و ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم

# %% md
####  الف. روی نمونه خودمان طبقه بندی انجام دهیم
# %%
#
df["median_house_value_categories"] = pd.cut(df["median_house_value"],
                                             bins=[0.0, 100000., 150000., 200000., 250000., 500001.],
                                             labels=[1, 2, 3, 4, 5])
df
# %%
df["median_house_value_categories"].value_counts()
#  کتگوری را بر اساس ماکسیمم مرتب میکنه
# %%
df["median_house_value_categories"].value_counts().sort_index().plot.bar(grid=True)
# عبارت sort_index() باعث میشه که دیگه بر اساس ماکسیمم نشان نده و براساس اندیس نشان دهد
# %%
# پیدا کردن مقدار Nan در y
print(train_set['median_house_value'].isnull().sum())
# اگر خروجی این کد عددی بزرگتر از 0 باشد، به این معنی است که ستون median_house_value در مجموعه داده آموزشی شما حاوی مقادیر NaN است
# وقتی شما متد .sum() را روی یک سری (Series) بولی اعمال می‌کنید، پایتون به طور خودکار مقادیر True را به 1 و مقادیر False را به 0 تبدیل می‌کند. سپس، این مقادیر 0 و 1 با هم جمع می‌شوند.
# %%
# جایگزین NaN با صفر در ترین  و تست
train_set['total_bedrooms'].fillna(0)
test_set['total_bedrooms'].fillna(0)

# # محاسبه میانه از مجموعه آموزش و جایگزین NaN با میانه در تست و ترین
# median_bedrooms_train = train_set['total_bedrooms'].median()
# train_set['total_bedrooms'].fillna(median_bedrooms_train)
# test_set['total_bedrooms'].fillna(median_bedrooms_train)
#
# # اگر می‌خواهید از میانگین استفاده کنید، به جای .median() از .mean() استفاده کنید
# mean_bedrooms_train = train_set['total_bedrooms'].mean()
# train_set['total_bedrooms'].fillna(mean_bedrooms_train)
# test_set['total_bedrooms'].fillna(mean_bedrooms_train)
# %% md

# %% md

# و ب.  تست و ترین را برای کتگوری مجددا از هم جدا میکنیم

# %%
str_train_set, str_test_set = train_test_split(df,
                                               test_size=0.2,
                                               stratify=df["median_house_value_categories"],
                                               random_state=40)
# df  همان X ما و stratify همان ستون ما است
#  که مقدار stratify را برابر با ستونی قرار میدهیم که رووش کتگوری ایجاد کردیم
# %%
str_train_set["median_house_value_categories"].value_counts() / len(str_train_set)
# %% md
"""
بیایید مرحله به مرحله بررسی کنیم:

    str_train_set["median_house_value_categories"].value_counts(): این قسمت از کد تعداد ردیف‌های مربوط به هر دسته منحصر به فرد در ستون median_house_value_categories را در مجموعه آموزش شمارش می‌کند. نتیجه یک سری (Series) است که در آن اندیس‌ها نام دسته‌ها و مقادیر تعداد نمونه‌های مربوط به هر دسته هستند.

    / len(str_train_set): این قسمت از کد نتیجه value_counts() را بر تعداد کل نمونه‌های موجود در مجموعه آموزش (len(str_train_set)) تقسیم می‌کند. این تقسیم باعث می‌شود که تعداد نمونه‌های هر دسته به یک نسبت یا درصد تبدیل شود.

مثال:

فرض کنید در مجموعه آموزش (str_train_set) 1000 نمونه وجود دارد و ستون median_house_value_categories به صورت زیر است:

    دسته 1: 200 نمونه
    دسته 2: 300 نمونه
    دسته 3: 250 نمونه
    دسته 4: 150 نمونه
    دسته 5: 100 نمونه

در این صورت:

    str_train_set["median_house_value_categories"].value_counts() نتیجه‌ای مشابه زیر خواهد داشت:

    1    200
    2    300
    3    250
    4    150
    5    100
    Name: median_house_value_categories, dtype: int64

    len(str_train_set) برابر با 1000 خواهد بود.

    فرمول str_train_set["median_house_value_categories"].value_counts()/len(str_train_set) نتایج زیر را به دست خواهد داد:

    1    0.20
    2    0.30
    3    0.25
    4    0.15
    5    0.10
    Name: median_house_value_categories, dtype: float64

    این نشان می‌دهد که:
        20% از نمونه‌های مجموعه آموزش در دسته 1 قرار دارند.
        30% از نمونه‌های مجموعه آموزش در دسته 2 قرار دارند.
        25% از نمونه‌های مجموعه آموزش در دسته 3 قرار دارند.
        15% از نمونه‌های مجموعه آموزش در دسته 4 قرار دارند.
        10% از نمونه‌های مجموعه آموزش در دسته 5 قرار دارند.

##### هدف از استفاده از این فرمول:

 ##### این فرمول معمولاً برای این موضوع استفاده می‌شود که آیا توزیع دسته‌ها در مجموعه آموزش با توزیع دسته‌ها در دیتافریم اصلی یا مجموعه آزمایش مطابقت دارد یا خیر.
  ##### این امر به ویژه زمانی مهم است که از روش stratify در train_test_split استفاده می‌کنید، 
 ##### زیرا هدف stratify این است که اطمینان حاصل شود که نسبت دسته‌ها در مجموعه‌های آموزش و آزمایش مشابه نسبت آن‌ها در دیتافریم اصلی است.
"""
# %% md
# ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم

# %%
str_train_set = str_train_set.drop("median_house_value_categories", axis=1)
# axis=1 یعنی ستون را در نظر بگیر
str_test_set = str_test_set.drop("median_house_value_categories", axis=1)
# %%
str_train_set
# %% md
 #_________________________________________________________________________________________________________________________________________
# Visualize
# %%
train = str_train_set
# %%
ax = sns.scatterplot(data=train,
                     x="longitude",
                     y="latitude",
                     size="population",
                     alpha=0.2,
                     hue="median_house_value")
sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
# %% md
 #_________________________________________________________________________________________________________________________________________
# Correlations
# %% md
"""
     ۱. کورلیشن (Correlation) در ماشین لرنینگ چیست؟

    کورلیشن (همبستگی) معیاری است که نشان می‌دهد چگونه دو متغیر با یکدیگر رابطه دارند.
    🔹 آیا وقتی مقدار یک متغیر زیاد شود، متغیر دیگر هم زیاد می‌شود؟
    🔹 آیا وقتی مقدار یک متغیر زیاد شود، مقدار متغیر دیگر کم می‌شود؟
    🔹 یا هیچ ارتباط مشخصی بین این دو متغیر وجود ندارد؟

    -----------------------------------------------------
کورلیشن در ماشین لرنینگ چه استفاده‌هایی دارد؟
📌 ۱. انتخاب ویژگی‌ها (Feature Selection)

    اگر دو ویژگی (مثلاً سن و قد) همبستگی بسیار زیادی داشته باشند، می‌توانیم یکی از آن‌ها را حذف کنیم.
    این کار باعث می‌شود مدل ساده‌تر و سریع‌تر شود.

📌 ۲. کشف روابط بین داده‌ها

    مثلاً در تحلیل داده‌های پزشکی، آیا مصرف زیاد نمک باعث افزایش فشار خون می‌شود؟
    مدل‌های یادگیری ماشین می‌توانند این ارتباطات را بررسی کنند.

📌 ۳. تشخیص مشکلات داده‌ها (Multicollinearity)

    اگر چند متغیر خیلی همبسته باشند، مدل ممکن است دچار مشکل شود و پیش‌بینی‌های نادرست بدهد.
    در این حالت، بهتر است ویژگی‌های همبسته را حذف کنیم.
"""
# %%
# train.corr()["median_house_value"]
# چون ستون ocean_proximity عدد نیست خطا میده
# %%
print(train.dtypes)

# %%
# محاسبه کورلیشن یا همبستگی (با در نظر نگرفتن ستونهای غیر عدد)
train.select_dtypes(include=["number"]).corr()["median_house_value"]
# این کد تضمین می‌کند که فقط اعداد در corr() استفاده شده‌اند.

# %%
sns.scatterplot(data=train,
                x="median_income",
                y="median_house_value",
                alpha=0.2)
# %% md
###### در شکل بالا خطهای افقی را میبینم که بهتره اینها حذف بشه احتمالا این اعداد دستی درست شدند
# %% md
# Attribute combinations

"""
Attribute Combinations یعنی ساخت ویژگی‌های جدید (New Features) از ترکیب ویژگی‌های موجود.
✔ هدف: کمک به مدل برای یادگیری بهتر الگوهای پنهان در داده‌

"""
# %%
# محاسبه نسبت یکی به دیگری
# همه اینکارا رو روی تست هم انجام میدیم اما فعلا ترین رو انجام دادیم
#  این نسبتهایی که داریم ایجاد میکنیم در واقع دارند یه ستون به جدولمون اضافه میکنه

# محاسبه اتاقهای یک خانه را در نظر بگیریم بهتر از اینه که کل اتاقها را در نظر بگیریم
train["rooms_per_house"] = train["total_rooms"] / train["households"]

#  نسبت اتاق خواب به کل اتاقها چقدر است
train["bedrooms_ratio"] = train["total_bedrooms"] / train["total_rooms"]

# یا مثلا درهر کدام از خانه ها چند نفر ادم هستند
train["people_per_house"] = train["population"] / train["households"]
train
# %%
# محاسبه کورلیشن یا همبستگی (با در نظر نگرفتن ستونهای غیر عدد)
train.select_dtypes(include=["number"]).corr()["median_house_value"]
# این کد تضمین می‌کند که فقط اعداد در corr() استفاده شده‌اند.

# %% md
 #_________________________________________________________________________________________________________________________________________
# Prepare Data
##### اماده کردن داده برای یادگیری مدلهای ماشین
# %%
train_features = train.drop("median_house_value", axis=1)
# ویژگی‌های مدل (Feature Matrix) یا همان X، یعنی متغیرهای مستقل

train_target = train["median_house_value"]
# برچسب هدف (Target Variable) یا همان  y، یعنی متغیر وابسته که مدل باید آن را پیش‌بینی کند

train_target
# %% md
## Data cleaning ==> using Pandas library
# %%
# train_features.info()
#  در خروجی entries یعنی تعداد سطرها
# %% md

# %%
# برای حذف کردن ستونی که نیاز نیست و مقدار Nan دارد
# remove rows with NA values
# train_features.dropna(subset=["total_bedrooms"]).info()
# %%
## train_features_with_imputed_na_values
# total_bedrooms_median = train_features["total_bedrooms"].median()
# train_features["total_bedrooms"] = train_features["total_bedrooms"].fillna(total_bedrooms_median)
# %%
# train_features.info()
# %%
# train_features["bedrooms_ratio"] = train_features["total_bedrooms"]/train_features["total_rooms"]
# %%
# train_features.info()
# %% md
 #_________________________________________________________________________________________________________________________________________
# Data cleaning ==> using slklearn library
# اصلاح دیتا _ برای یادگیری ماشین _ توسط کتابخانه سایکیت لرن

# %% md
# 1
# sklearn impute
# %%
from sklearn.impute import SimpleImputer

# %%
simple_imputer = SimpleImputer(strategy="median")
# استراتژی را تعیین میکنیم که مثلا جای مقادیر نن، میانه بگذاریم
# توجه شود که فعلا مدل را ساختیم و هنوز داده ای بهش ندادیم
# %%
train_features = train.drop("median_house_value", axis=1)
# ویژگی‌های مدل (Feature Matrix) یا همان X، یعنی متغیرهای مستقل

train_target = train["median_house_value"]
# برچسب هدف (Target Variable) یا همان  y، یعنی متغیر وابسته که مدل باید آن را پیش‌بینی کند

train_features.info()
# %%
# مهم : چون بعضی ستونها عددی نیست فقط ستونهایی که عدد هستند را انتخاب میکنیم
train_features_numeric = train_features.select_dtypes(include=[np.number])
train_features_numeric.info()
# %%
simple_imputer.fit(train_features_numeric)
# مقدار جایگزین نن را محاسبه و ذخیره میکند
# %%
# عبارت یک اتریبوت برای ایمپیوت  است که  statistics_
# نمایش مقدارهی جایگزین شده برای هر ستون
simple_imputer.statistics_

# %%
# مقدارهای نن را با مقدار محاسبه شده جایگزین میکند
simple_imputer.transform(train_features_numeric).shape

# 💡 پس statistics_ فقط مقدارهایی که fit() یاد گرفته را نشان می‌دهد، ولی transform() واقعاً داده‌ها را تغییر می‌دهد.
# %%
# این کد داده‌های عددی train_features_numeric رو که مقادیر از‌دست‌رفته (NaN) داشتن، با مقدار میانه (یا هر استراتژی دیگری که در simple_imputer تنظیم کردیم) جایگزین می‌کنه و دوباره در یک دیتافریم جدید ذخیره می‌کنه.
train_features_numeric = pd.DataFrame(simple_imputer.transform(train_features_numeric),
                                      columns=train_features_numeric.columns,
                                      index=train_features_numeric.index)
# دللیل دادن کالمنز و ایندکس این است که ستون و ایندکس را از دیتافرم اصلی بگیرد
# و اینکار باعث میشه که ردیف های جدید همان نام ستون و ایندکس قبلی را داشته باشد

# %%
train_features_numeric
# %% md
# Consistency
# estimator
# transformer
# predictors

# Inspection

# Composition
# %% md
"""
✅  Consistency (یکنواختی) در scikit-learn چیست؟

    🔹 scikit-learn یک استاندارد مشخص برای کلاس‌های مختلف یادگیری ماشین دارد تا کار کردن با آن‌ها راحت و قابل پیش‌بینی باشد. این ویژگی را Consistency (یکنواختی) می‌نامند.
    🔹 یعنی همه مدل‌ها (estimators، transformers، predictors) یک ساختار کلی دارند و از متدهای یکسانی پیروی می‌کنند.

    📌 ویژگی‌های Consistency:
    ✔ تمام مدل‌ها (estimators) دارای fit() هستند (آموزش مدل) .
    بعضی estimators ها میتوانند ترنسفرم هم باشند مثل  SimpleImputer . درسته؟  
    ✔ مدل‌های پیش‌بینی (predictors) دارای predict() هستند (پیش‌بینی خروجی)
    ✔ مدل‌های تبدیلی (transformers) دارای transform() هستند (تبدیل داده‌ها)
    ✔ هر مدل مجموعه‌ای از hyperparameters دارد که در هنگام ایجاد شیء تنظیم می‌شوند.


✅ 1. estimator (تخمین‌گر) چیست؟

    Estimator به هر مدل یادگیری ماشین در scikit-learn گفته می‌شود که دارای fit() است.
    ✔ مدل‌های رگرسیون، طبقه‌بندی، و خوشه‌بندی همگی estimator هستند.
    ✔ مثال: LinearRegression, RandomForestClassifier, KMeans


✅ 2. transformer (تبدیل‌کننده) چیست؟

    🔹 transformer مدلی است که روی داده‌ها پردازش انجام می‌دهد و آن‌ها را تغییر می‌دهد، ولی پیش‌بینی (predict()) انجام نمی‌دهد.
    🔹 همه transformers دارای متد fit_transform() هستند.

    ✔ مثال‌ها:
        StandardScaler: داده‌ها را استاندارد می‌کند.
        PCA: تعداد ویژگی‌ها را کاهش می‌دهد.
        SimpleImputer: مقدارهای NaN را جایگزین می‌کند.

    ✅ نتیجه:
    StandardScaler یک transformer است که داده‌ها را استاندارد می‌کند.
    چون پیش‌بینی انجام نمی‌دهد، predict() ندارد.


✅ 3. predictor (پیش‌بینی‌کننده) چیست؟

    🔹 predictor به مدلی گفته می‌شود که می‌تواند روی داده‌های جدید پیش‌بینی انجام دهد.
    🔹 همه predictors دارای predict() هستند.

    ✔ مثال‌ها:
        رگرسیون: LinearRegression, SVR
        طبقه‌بندی: LogisticRegression, RandomForestClassifier


✅ 4. Inspection (بازرسی مدل) چیست؟

    🔹 Inspection یعنی قابلیت بررسی و تحلیل ویژگی‌های مدل بعد از آموزش.
    🔹 scikit-learn ابزارهایی ارائه می‌دهد تا بفهمیم مدل چگونه تصمیم می‌گیرد.

    ✔ مثال‌ها:
        coef_ در LinearRegression → نشان می‌دهد کدام ویژگی‌ها مهم‌ترند.
        feature_importances_ در RandomForest → میزان اهمیت هر ویژگی در مدل را نشان می‌دهد.

    ✅ نتیجه:
        نشان می‌دهد که کدام متغیرها بیشتر در پیش‌بینی نقش دارند


✅ 5. Composition (ترکیب مدل‌ها) چیست؟

    🔹 Composition یعنی ترکیب چندین مدل یادگیری ماشین برای بهبود عملکرد.
    🔹 دو نوع روش مهم در Composition:
    1️⃣ Ensemble Learning (یادگیری گروهی): ترکیب چندین مدل برای بهبود دقت

        مثال‌ها: RandomForest, AdaBoost, VotingClassifier
        2️⃣ Pipeline (لوله پردازشی): 

"""
# %% md
 #_________________________________________________________________________________________________________________________________________
# categorical features

"""
     - ویژگی‌های دسته‌ای (Categorical Features) متغیرهایی هستند که مقدار آن‌ها به دسته‌های مشخصی تعلق دارد و مقدارهای عددی ندارند.
     -این ویژگی‌ها نشان‌دهنده‌ی برچسب‌ها یا گروه‌های مجزا هستند، نه مقادیر پیوسته.
     -این مقادیر اعداد نیستند، پس مدل یادگیری ماشین نمی‌تواند مستقیم از آن‌ها استفاده کند.
     -پس باید آن‌ها را به روشهایی که در زیر گفته میشه به عدد تبدیل کنیم.


-انواع ویژگی‌های دسته‌ای (Categorical Data Types):

    دو دسته وجود دارد: داده‌های Nominal هیچ رابطه‌ی ترتیبی بین دسته‌ها وجود ندارد، اما در داده‌های Ordinal، ترتیب مهم است.


-چرا باید ویژگی‌های دسته‌ای را تبدیل کنیم؟

    مدل‌های یادگیری ماشین مانند رگرسیون خطی و جنگل تصادفی فقط می‌توانند با داده‌های عددی کار کنند.
    پس اگر ویژگی‌های دسته‌ای را مستقیماً به مدل بدهیم، کار نمی‌کند!


-روش‌های تبدیل ویژگی‌های دسته‌ای به عدد:

     ۱. Label Encoding (رمزگذاری برچسبی - مخصوص دیتای اوردینال)
    ۲. One-Hot Encoding (  رمزگذاری تک‌داغ - مخصوص داده های نامینال)
     ۳. Ordinal Encoding (رمزگذاری ترتیبی - مخصوص اوردینال که ترتیب دست خودت )
    ۴. Target Encoding (رمزگذاری بر اساس مقدار هدف - مخصوص دیتاهای زیاد و مدلهای پیچیده)

"""
# %%
train_features[["ocean_proximity"]].value_counts()
# %%
# این کد ستون ocean_proximity رو از train_features جدا می‌کنه و یک دیتافریم جدید به نام train_features_categorical می‌سازه.
train_features_categorical = train_features[["ocean_proximity"]]
# %% md

# OrdinalEncoder
# %%
from sklearn.preprocessing import OrdinalEncoder

# %%
oe = OrdinalEncoder()
ocean_proximity_index = oe.fit_transform(train_features_categorical)
ocean_proximity_index

# نکته مهم : البته باید توجه داشت که OrdinalEncoder برای داده های این ستون که اوردینال نیستند و نامینال هستند مناسب نیست
# چون پایتون برای اعداد بزرگتر ممکنه ارزش دیگری قائل شود


# نکته مهم : #  در واقع fit_transform فیت و ترنسفورم را با هم انجام میده
# حتما توجه شود که fit_transform فقط برای ترین هستش
# و برای تست حتما اول باید فیت کرد و بعد اگر خواستی ترنسفورم کرد
# چون در غیر اینصورت انگار توضیح را برای تست هم دادی که دیگه تست برای مدل ناشناخته نیست

# %%
train_features_categorical
# %%
# اینجا مشخص میکنیم که اعداد را به ترتیب به کدام یک از دسته ها داده
# توجه شود که اولی میشه صفر و به همین ترتیب جلو میره
oe.categories_
# %% md
# OneHotEncoder
# %%
from sklearn.preprocessing import OneHotEncoder

# %%
ohe = OneHotEncoder()
ocean_proximity_ohe = ohe.fit_transform(train_features_categorical)

# در واقع fit_transform فیت و ترنسفورم را با هم انجام میده
# حتما توجه شود که fit_transform فقط برای ترین هستش
# و برای تست حتما اول باید فیت کرد و بعد اگر خواستی ترنسفورم کرد
# چون در غیر اینصورت انگار توضیح را برای تست هم دادی که دیگه تست برای مدل ناشناخته نیست

# %%
ocean_proximity_ohe.toarray()
# %%
ohe.categories_
# %%
# این متد اسم ستونها را بهمون میده
ohe.get_feature_names_out()
# %%
# حالا دیگه با مدل OneHotEncoder به جای اسامی اعداد داریم
train_features_categorical_ohe = pd.DataFrame(ocean_proximity_ohe.toarray(),
                                              columns=ohe.get_feature_names_out())
# %% md
 #_________________________________________________________________________________________________________________________________________
"""
داده های پرت:

    داده‌های اوت لیر (Outlier Data) به انگلیسی Outlier Data یا به طور ساده‌تر Outliers

مثال:

    مثلا فرض کنید مثلا یه تعداد زیادی داده بین 1 تا 100 دارید
    و بعد  چند مقدار هم دارید که 500
    حالا وقتی مین ماکس اسکلر را انجام میدیم و مثلا بازه 1 تا -1 درنظر گرفتیم یه تعداد زیادی از بازه -1 تا 1 مقادیر زیادی دادهنداریم ولی جای دیگر از بازه تعداد زیادی داده داریم
"""

# %% md
 #_________________________________________________________________________________________________________________________________________
"""
- داشتن دیتا اسکیلرهای زیاد ممکن است باعث عملکرد نامناسب شود.
- 
            فرض کنید سه ستون در مجموعه داده شما وجود دارد که مقادیر آن‌ها در بازه‌های مختلف قرار دارند:

            ستون A: مقادیر بین 0 تا 15
            ستون B: مقادیر بین 20,000 تا 40,000
            ستون C: مقادیر بین 2,000 تا 6,000

            اگر از یک الگوریتم یادگیری ماشین استفاده کنید که به فاصله یا اندازه حساس است (مانند KNN، SVM، شبکه‌های عصبی)، ستون B به دلیل داشتن مقادیر بسیار بزرگ‌تر، تاثیر بسیار بیشتری بر روی محاسبات خواهد داشت. به عبارت دیگر، الگوریتم به تغییرات در ستون B اهمیت بیشتری می‌دهد، حتی اگر تغییرات در ستون A یا C از نظر اهمیت واقعی در داده‌ها بیشتر باشند.


- دیتا اسکیلرها انواع مختلفی دارند، از جمله:

        MinMaxScaler: مقادیر را به یک بازه مشخص (معمولاً 0 تا 1) مقیاس‌بندی می‌کند.
        StandardScaler: مقادیر را به گونه‌ای تغییر می‌دهد که میانگین آن صفر و انحراف معیار آن یک شود.
        RobustScaler: نسبت به داده‌های پرت (Outliers) مقاوم‌تر است.
        - 

تفاوت‌های کلیدی:

    - MinMaxScaler:
        روش: مقادیر را خطی به یک بازه مشخص، معمولاً بین 0 و 1، مقیاس‌بندی می‌کند.
        مزایا: حفظ شکل توزیع داده‌ها، محدود کردن مقادیر به یک بازه مشخص.
        معایب: حساسیت به داده‌های پرت.
        کاربرد: زمانی که نیاز به محدود کردن بازه مقادیر است، یا زمانی که توزیع داده‌ها مهم است و داده‌های پرت زیادی وجود ندارند.

    - StandardScaler:
        روش: مقادیر را به گونه‌ای تغییر می‌دهد که میانگین آن صفر و انحراف معیار آن یک شود.
        مزایا: مناسب برای الگوریتم‌های حساس به فاصله، بهبود همگرایی برخی الگوریتم‌ها، کاهش تاثیر داده‌های پرت (نسبت به MinMaxScaler).
        معایب: حساسیت به داده‌های پرت (اگرچه کمتر از MinMaxScaler)، بازه مقادیر محدود نمی‌شود.
        کاربرد: زمانی که الگوریتم به فاصله حساس است، داده‌ها تقریباً توزیع نرمال دارند و داده‌های پرت زیادی وجود ندارند.

    - RobustScaler:
        روش: از میانه و چارک‌ها برای مقیاس‌بندی استفاده می‌کند و نسبت به داده‌های پرت مقاوم‌تر است.
        مزایا: مقاومت بالا در برابر داده‌های پرت.
        معایب: بازه مقادیر محدود نمی‌شود، ممکن است برای داده‌های بدون پرت عملکرد خوبی نداشته باشد.
        کاربرد: زمانی که مجموعه داده شامل تعداد زیادی داده پرت است.

انتخاب روش مناسب:
انتخاب بهترین روش مقیاس‌بندی بستگی به ویژگی‌های داده‌ها و نوع الگوریتم یادگیری ماشین مورد استفاده دارد.

    اگر داده‌های پرت زیادی دارید، RobustScaler انتخاب مناسب‌تری است.
    اگر الگوریتم شما به فاصله حساس است و داده‌های پرت زیادی ندارید، StandardScaler ممکن است بهتر باشد.
    اگر نیاز به محدود کردن بازه مقادیر دارید یا داده‌های پرت زیادی ندارند، MinMaxScaler می‌تواند مناسب باشد.


نکته مهم:

        در عمل، ممکن است لازم باشد چندین روش را امتحان کنید تا ببینید کدام یک بهترین عملکرد را برای مدل شما ارائه می‌دهد. مهم است که به یاد داشته باشید که اسکیلر باید فقط بر روی داده‌های آموزشی فیت شود و
         سپس از همان اسکیلر برای تبدیل داده‌های تست استفاده شود تا از نشت اطلاعات جلوگیری شود.

"""
# %% md
 #_________________________________________________________________________________________________________________________________________
"""
# MinMaxScaler 
# Normalization ## in sklearn library
به طور خلاصه: داده ها را بین صفر و یک میبره
نرمالسازی کردن یا نرمالیزشن اینجا به معنای توزیع نرمال نیست 

"""

"""
-  موارد مفید بودن MinMaxScaler:

        بهبود عملکرد الگوریتم‌های حساس به مقیاس: بسیاری از الگوریتم‌های یادگیری ماشین، به ویژه آن‌هایی که بر پایه فاصله (مانند KNN، SVM با کرنل RBF) یا گرادیان (مانند شبکه‌های عصبی) کار می‌کنند، به مقیاس ویژگی‌ها حساس هستند. اگر ویژگی‌ها مقیاس‌های متفاوتی داشته باشند، ویژگی‌هایی با مقادیر بزرگ‌تر می‌توانند تاثیر بیشتری بر روی نتیجه الگوریتم بگذارند و باعث عملکرد نامناسب شوند. MinMaxScaler با نرمال‌سازی داده‌ها، این مشکل را کاهش می‌دهد.

        جلوگیری از سرریز شدن محاسبات: در برخی موارد، مقادیر بسیار بزرگ در ویژگی‌ها می‌توانند منجر به سرریز شدن محاسبات در الگوریتم‌ها شوند. MinMaxScaler با محدود کردن مقادیر به یک بازه مشخص، از این مشکل جلوگیری می‌کند.

        تسریع فرآیند همگرایی: برای الگوریتم‌هایی مانند گرادیان نزولی که در شبکه‌های عصبی استفاده می‌شوند، مقیاس‌بندی ویژگی‌ها می‌تواند به تسریع فرآیند همگرایی و رسیدن به پاسخ بهینه کمک کند.

        تفسیرپذیری بهتر (در برخی موارد): وقتی ویژگی‌ها در یک بازه مشخص (مثلاً 0 تا 1) قرار می‌گیرند، ممکن است تفسیر آن‌ها آسان‌تر شود، به خصوص اگر بخواهید اهمیت نسبی ویژگی‌ها را بررسی کنید.


- معایب استفاده از MinMaxScaler:

        حساسیت به داده‌های پرت (Outliers): اگر داده‌های پرت زیادی در مجموعه داده وجود داشته باشند، MinMaxScaler می‌تواند تحت تأثیر آن‌ها قرار گیرد و باعث شود که سایر داده‌ها در یک بازه بسیار کوچک فشرده شوند.
        عدم مدیریت داده‌های گمشده: MinMaxScaler به طور مستقیم نمی‌تواند داده‌های گمشده را مدیریت کند و قبل از استفاده از آن، باید داده‌های گمشده را با روش‌های مناسب مدیریت کرد.


- کاربردهای MinMaxScaler:

        پیش‌پردازش داده‌ها برای الگوریتم‌های یادگیری ماشین: این رایج‌ترین کاربرد MinMaxScaler است. قبل از آموزش مدل‌های یادگیری ماشین، به خصوص مدل‌های حساس به مقیاس، از MinMaxScaler برای نرمال‌سازی داده‌ها استفاده می‌شود.

        پردازش تصاویر: در پردازش تصاویر، مقادیر پیکسل‌ها معمولاً بین 0 و 255 هستند. MinMaxScaler می‌تواند برای نرمال‌سازی این مقادیر به بازه 0 تا 1 استفاده شود، که می‌تواند به بهبود عملکرد مدل‌های بینایی کامپیوتر کمک کند.

        پردازش سیگنال: در پردازش سیگنال، MinMaxScaler می‌تواند برای نرمال‌سازی دامنه سیگنال‌ها استفاده شود.

        تحلیل داده‌های مالی: در تحلیل داده‌های مالی، ممکن است از MinMaxScaler برای نرمال‌سازی قیمت سهام یا سایر شاخص‌های مالی استفاده شود تا بتوان آن‌ها را بهتر مقایسه کرد یا در مدل‌های پیش‌بینی استفاده کرد.

        هر جایی که نیاز به محدود کردن مقادیر به یک بازه مشخص باشد: به طور کلی، هر جا که نیاز باشد مقادیر یک ویژگی را به یک بازه مشخص محدود کنید (بدون تغییر شکل توزیع)، MinMaxScaler می‌تواند مفید باشد.

مثال:

        تصور کنید دو ویژگی دارید: سن (که معمولاً بین 0 تا 100 است) و درآمد (که می‌تواند بسیار بزرگ باشد). اگر از یک الگوریتم مبتنی بر فاصله استفاده کنید، درآمد به دلیل مقادیر بزرگ‌تر، تاثیر بسیار بیشتری نسبت به سن خواهد داشت. استفاده از MinMaxScaler هر دو ویژگی را به بازه 0 تا 1 می‌برد و باعث می‌شود که هر دو ویژگی تاثیر متناسبی در الگوریتم داشته باشند.

"""
# %%
from sklearn.preprocessing import MinMaxScaler

# %%
mms = MinMaxScaler(feature_range=(-1, 1))
# %%
mms.fit_transform(train_features_numeric)
# %% md
 #_________________________________________________________________________________________________________________________________________
"""
# StandardScaler
#یک روش نرمال سازی داده هاست که
 #که میانگین داده‌ها را صفر و واریانس آن‌ها را یک می‌کند.

z = (x - u) / s
u میانگین 
s انحراف معیار

# Standard Deviation استاندارد دیویشن :
      همان انحراف معیار است .
      یک معیار آماری است که نشان می‌دهد داده‌ها چقدر از میانگین (mean) پراکنده شده‌اند.


"""
# %% md
"""
-  استاندارد کردن داده یعنی داده را میبره به میانگین صفر و انحراف استاندارد یک
- اما استانداردسازی شکل توزیع داده‌ها را تغییر نمی‌دهد.
 - یعنی اگر داده‌ها ابتدا توزیع نرمال نداشته باشند، بعد از استانداردسازی هم نرمال نخواهند شد

-  استاندارد دیویشن بالا نشان می‌دهد که داده‌ها به طور کلی از میانگین دور هستند و پراکندگی زیادی دارند.
-  استاندارد دیویشن پایین نشان می‌دهد که داده‌ها به طور کلی نزدیک به میانگین هستند و پراکندگی کمی دارند.



مزایای استفاده از استاندارد اسکالر:

    مناسب برای الگوریتم‌های حساس به فاصله: استاندارد اسکالر به خوبی برای الگوریتم‌هایی کار می‌کند که به فاصله بین نقاط داده حساس هستند، مانند ماشین‌های بردار پشتیبان (اس وی ام)، رگرسیون لجستیک و شبکه‌های عصبی.
    مدیریت داده‌های با توزیع نرمال: اگر داده‌های شما تقریباً توزیع نرمال داشته باشند، استاندارد اسکالر می‌تواند به بهبود عملکرد مدل کمک کند.
    کاهش تاثیر داده‌های پرت (تا حدودی): اگرچه استاندارد اسکالر به اندازه RobustScaler در برابر داده‌های پرت مقاوم نیست، اما همچنان می‌تواند تاثیر آن‌ها را کاهش دهد.

معایب استفاده از استاندارد اسکالر:

    حساسیت به داده‌های پرت: داده‌های پرت می‌توانند میانگین و انحراف معیار را به شدت تحت تاثیر قرار دهند و در نتیجه، مقادیر مقیاس‌بندی شده نیز تحت تاثیر قرار گیرند.
    عدم محدود کردن بازه مقادیر: برخلاف MinMaxScaler که مقادیر را به یک بازه مشخص محدود می‌کند، استاندارد اسکالر بازه مقادیر را محدود نمی‌کند. این ممکن است در برخی موارد مشکل‌ساز باشد.

چه زمانی از استاندارد اسکالر استفاده کنیم؟

    زمانی که الگوریتم یادگیری ماشین شما به فاصله بین نقاط داده حساس است.
    زمانی که داده‌های شما تقریباً توزیع نرمال دارند.
    زمانی که داده‌های پرت زیادی در مجموعه داده وجود ندارند یا تاثیر آن‌ها را می‌پذیرید.

# نکته مهم:
        اگر هدف اصلی شما بهبود عملکرد الگوریتم‌های حساس به مقیاس و توزیع است، استاندارد اسکالر اغلب انتخاب بهتری است، حتی اگر داده‌ها از قبل توزیع نرمال داشته باشند. با این حال، اگر نیاز به حفظ شکل توزیع دارید یا داده‌های پرت زیادی وجود دارند، مین‌مکس اسکالر ممکن است مناسب‌تر باشد. 
"""
# %%
from sklearn.preprocessing import StandardScaler

# %%
ss = StandardScaler()
# %%
ss.fit_transform(train_features_numeric)
# %% md
 #_________________________________________________________________________________________________________________________________________
#  لگاریتم برای داده ها با چوله راست یا دم راست
# به جای مین ماکس اسکیلر یا استاندارد اسکیلر

"""
لگاریتم گرفتن از داده‌ها همیشه توزیع را نرمال نمی‌کند، اما در بسیاری از موارد می‌تواند داده‌هایی که چوله به راست (right-skewed) هستند را به توزیع نزدیک‌تر به نرمال تبدیل کند.

🔹 چه زمانی لگاریتم گرفتن مفید است؟

    ✅ زمانی که داده‌ها توزیع چوله به راست دارند (یعنی مقادیر بزرگ بسیار زیاد هستند).
    ✅ زمانی که مقدار متغیرها بازه بزرگی دارند و لازم است تأثیر مقادیر بزرگ کاهش یابد.
    ✅ زمانی که توزیع داده‌ها دارای دم سنگین (heavy-tailed) باشد.


🔹 چه زمانی لگاریتم گرفتن کمکی نمی‌کند؟

    ❌ اگر داده‌ها چوله به چپ باشند (چوله به سمت مقادیر کوچک)، لگاریتم گرفتن معمولاً کمکی نمی‌کند.
    ❌ اگر داده‌ها توزیع دوبخشی (bimodal) یا چندبخشی (multimodal) داشته باشند، لگاریتم نمی‌تواند آن را نرمال کند. (یعنی وقتی نمودار را نگاه میکنیم دو یا بیشتر زنگوله وجود دارد)
    ❌ اگر داده‌ها دارای مقادیر منفی یا صفر باشند، مستقیماً نمی‌توان از تابع لگاریتم معمولی استفاده کرد (چون log⁡(x)log(x) برای x≤0x≤0 تعریف نشده است). در این صورت، می‌توان log⁡(x+c)log(x+c) را به کار برد که cc یک مقدار ثابت مثبت است.
    🔹 جایگزین‌های لگاریتم برای نرمال‌سازی داده‌ها

اگر لگاریتم گرفتن توزیع را نرمال نکند، می‌توان روش‌های زیر را امتحان کرد:

    ✅ تبدیل Box-Cox: برای داده‌های مثبت و چوله به راست مناسب است.
    ✅ تبدیل Yeo-Johnson: برای داده‌های دارای مقادیر منفی و صفر هم کار می‌کند.
    ✅ ریشه‌گیری (مثلاً xx
    ​ یا x1/3x1/3): گاهی می‌تواند چولگی را کاهش دهد.
"""
# %% md
# میخواهیم دو نمودار را با و بدون لگاریتم ببینیم
# %%
train_features_numeric["total_rooms"].hist(bins=30)
# %%
train_features_numeric["total_rooms"].apply(np.log).hist(bins=30)
# %% md
 #_________________________________________________________________________________________________________________________________________
"""
# مالتی مودال ها
 #### برای فیچرها با ستونهایی که نمودارشون به صورت مالتی مودال هستند چه کنیم:
یک روش:
     استفاده از پیدی کات (که متغیر پیوسته را بازه بازه میکینم)
    فقط حواستون باشه بعد که کتگوری کتگوری شده انها را به عدد تبدیل کنید 
    حالا یا بااستفاده از وان هات انکدینگ یا ...

روش دیگر: 
    مد کاکسیمم فیچر را در نظر میگیریم
    اینطوری یه فیچری میسازیم که درواقع میاد فاصله بقیه نقاط را از این مد ماکسیمم نشون میده
    که اصطلاحا بهش سیمیلاریتی مژر گویند که یک معیاری می سازیم برای شباهت نقاط از مد ماکسیمم
    که اگر از مد ماکسیمم دور شویم اون شباهت کمتر میشه
    یک تابعی  که همچین کاری را انجام میده  rbf_kernel است
    هر تابعی که فقط به فاصله بین مقدار ورودی و یک مقدار فیکس وابسته باشه میتونه اینکار برامون انجام بده

"""

# %% md
 #_________________________________________________________________________________________________________________________________________
# rbf_kernel
# %% md
"""
🔹 rbf_kernel
🔸 نام کامل انگلیسی: Radial Basis Function Kernel
🔸 ترجمه فارسی: هسته تابع پایه شعاعی


🔸 کاربرد: تابعی که برای اندازه‌گیری شباهت بین دو نقطه در فضا استفاده می‌شود و در ماشین‌های بردار پشتیبان (SVM)، یادگیری ماشین و کاهش بعد کاربرد دارد.
در یادگیری ماشین و ریاضیات، وقتی می‌گوییم که تابع rbf_kernel برای اندازه‌گیری شباهت بین دو نقطه در فضا استفاده می‌شود، یعنی این تابع بررسی می‌کند که دو بردار (نقطه) چقدر به هم نزدیک یا شبیه هستند.

    به طور ساده‌تر:
    اگر دو نقطه خیلی نزدیک باشند، مقدار rbf_kernel عددی نزدیک به ۱ خواهد شد.
    اگر دو نقطه دور از هم باشند، مقدار rbf_kernel عددی نزدیک به ۰ می‌شود


🔹 تابع RBF چیست؟

    هسته تابع پایه شعاعی (RBF) یک تابع غیرخطی است که فاصله بین دو نقطه را به گونه‌ای تبدیل می‌کند که نقاط نزدیک‌تر به هم مقدار شباهت بیشتری داشته باشند و نقاط دورتر مقدار شباهت کمتری. این تابع معمولاً در یادگیری ماشین برای نقشه‌برداری داده‌ها به یک فضای بُعد بالاتر استفاده می‌شود تا الگوهای غیرخطی را بهتر شناسایی کند.
     K(x, y) = exp(-gamma ||x-y||^2)

آیا rbf_kernel بعد داده‌ها را افزایش می‌دهد؟

    وقتی می‌گوییم که "این تابع داده‌ها را به یک فضای بُعد بالاتر نگاشت می‌کند"، به این معنی نیست که همیشه فقط یک بُعد اضافه می‌شود. بلکه rbf_kernel داده‌ها را از فضای اصلی‌شان به یک فضای جدید، که می‌تواند خیلی بُعدی باشد، نگاشت می‌کند.
    چرا این اتفاق می‌افتد؟

    برخی مسائل یادگیری ماشین، مثل SVM، وقتی داده‌ها را در فضای اصلی بررسی می‌کنند، ممکن است خطی جداشدنی نباشند (یعنی نتوان با یک خط ساده آن‌ها را دسته‌بندی کرد). اما اگر داده‌ها را به یک فضای بُعد بالاتر ببریم، ممکن است بتوانیم با یک مرز خطی آن‌ها را جدا کنیم.

     مثال ساده:
    فرض کنید نقاط زیر را در صفحه دو‌بُعدی داریم:

    نقطه‌های کلاس A: (1, 1) و (2, 2)
    نقطه‌های کلاس B: (5, 5) و (6, 6)

    اگر این نقاط را در فضای دو‌بعدی ببینیم، ممکن است نتوانیم به‌راحتی یک خط بکشیم که کلاس‌ها را جدا کند. اما اگر از rbf_kernel استفاده کنیم، این نقاط به یک فضای با ابعاد بالاتر تبدیل می‌شوند که در آنجا ممکن است خطی برای جداسازی وجود داشته باشد.


🔹 آیا همیشه یک بعد اضافه می‌شود؟

    نه! تعداد ابعاد جدید به تعداد ویژگی‌ها و ساختار داده‌ها بستگی دارد. در واقع، RBF داده‌ها را به یک فضای با بی‌نهایت بُعد (فضای هیلبرت) نگاشت می‌کند، اما در عمل این فضا تأثیر ریاضی دارد و مدل نیازی ندارد که به‌صورت صریح این فضا را محاسبه کند.


🔹 چرا از RBF Kernel استفاده می‌کنیم؟

    ✅ مدل‌سازی روابط غیرخطی: اگر داده‌ها به‌صورت خطی جداشدنی نباشند، RBF به یافتن الگوهای پیچیده کمک می‌کند.
    ✅ کارایی بالا در بعد بالا: برخلاف برخی روش‌ها، RBF در فضاهای با ابعاد بالا خوب عمل می‌کند.
    ✅ مقاوم در برابر نویز: چون وابسته به فاصله بین داده‌هاست، نویزهای کوچک تأثیر زیادی روی آن ندارند.


🔹 کاربردهای rbf_kernel در یادگیری ماشین

    SVM (ماشین بردار پشتیبان): وقتی داده‌ها خطی جداشدنی نیستند، RBF Kernel آن‌ها را به فضای بُعد بالاتر نگاشت می‌کند تا جداپذیر شوند.
    Kernels in Clustering ( 并 خوشه‌بندی ): در روش‌هایی مثل Spectral Clustering از rbf_kernel برای محاسبه شباهت بین داده‌ها استفاده می‌شود.
    Dimensionality Reduction (کاهش بُعد): در روش‌هایی مثل Kernel PCA از rbf_kernel برای یافتن فضاهای مناسب کاهش بعد استفاده می‌شود.
"""
# %%
from sklearn.metrics.pairwise import rbf_kernel

# %%
rbf_kernel(train_features_numeric[["housing_median_age"]], [[35]], gamma=0.1)
# عبارت  [[35]]: این بخش، مقدار ثابت 35 را به عنوان نقطه مرجع تعیین می‌کند.
# هدف این است که شباهت هر میانگین سن خانه را با سن 35 سال محاسبه کنیم.

# عبارت gamma=0.1: این بخش، مقدار گاما را 0.1 تعیین می‌کند. این مقدار کم گاما نشان می‌دهد که تاثیر نقاط داده بر روی محاسبه شباهت، گسترده‌تر است. یعنی خانه‌هایی که حتی میانگین سنی نسبتاً دور از 35 سال دارند، باز هم تاثیر قابل توجهی بر روی نتیجه کرنل خواهند داشت.

# مثال از گاما:
# فرض کنید میانگین سن خانه‌ها در سه منطقه به ترتیب 30، 35 و 40 سال باشد.
#
#     اگر گاما 0.1 باشد، شباهت سن‌های 30 و 40 سال با 35 سال، نسبتاً بالا خواهد بود، زیرا تاثیر نقاط دورتر نیز در نظر گرفته می‌شود.
#     اگر گاما 10 باشد، شباهت سن 35 سال با خودش بسیار بالا خواهد بود، اما شباهت سن‌های 30 و 40 سال با 35 سال، بسیار پایین خواهد بود، زیرا فقط نقاط بسیار نزدیک به 35 سال تاثیر قابل توجهی دارند.
# %%
fig, ax = plt.subplots()
ax.hist(train_features_numeric["housing_median_age"], bins=30)
# یک هیستوگرام(histogram) از ستون"housing_median_age" در DataFrame train_features_numeric رسم می‌کند.
# bins=30 تعداد سطل‌های هیستوگرام را تعیین می‌کند.

# لازمه که برای rbf_kernel ای که در کد قبل نوشتیم، محور ایکس بسازیم؛ چون rbf_kernelانگار محورy است
hosing_ages = np.linspace(train_features_numeric["housing_median_age"].min(),
                          train_features_numeric["housing_median_age"].max(),
                          1000).reshape(-1, 1)
# عبارت np.linspace() میاد 1000 نقطه را به طور مساوی بین حداقل و حداکثر مقادیر ستون "housing_median_age" ایجاد می‌کند.
# .reshape(-1, 1) آرایه را به یک آرایه دوبعدی با 1000 ردیف و 1 ستون تبدیل می‌کند. این کار برای استفاده از rbf_kernel ضروری است.

rbf1 = rbf_kernel(hosing_ages, [[35]], gamma=0.1)
# این خط کرنل RBF را بین hosing_ages و مقدار ثابت [[35]] با گاما 0.1 محاسبه می‌کند.

ax2 = ax.twinx()
# یعنی انگار داریم محور ایکس را تکرار میکنمی
# یعنی محور وای که ایجاد میشه روی محور ایکس قبلی اجرا میشه. یعنی محور ایکس بین دو تابع ax1 , ax2 مشترک است.

ax2.plot(hosing_ages, rbf1, color="black", lw=2)
# %%
fig, ax = plt.subplots()
ax.hist(train_features_numeric["housing_median_age"], bins=30)
# یک هیستوگرام(histogram) از ستون"housing_median_age" در DataFrame train_features_numeric رسم می‌کند.
# bins=30 تعداد سطل‌های هیستوگرام را تعیین می‌کند.

# لازمه که برای rbf_kernel ای که در کد قبل نوشتیم، محور ایکس بسازیم؛ چون rbf_kernelانگار محورy است
hosing_ages = np.linspace(train_features_numeric["housing_median_age"].min(),
                          train_features_numeric["housing_median_age"].max(),
                          1000).reshape(-1, 1)
# عبارت np.linspace() میاد 1000 نقطه را به طور مساوی بین حداقل و حداکثر مقادیر ستون "housing_median_age" ایجاد می‌کند.
# .reshape(-1, 1) آرایه را به یک آرایه دوبعدی با 1000 ردیف و 1 ستون تبدیل می‌کند. این کار برای استفاده از rbf_kernel ضروری است.

rbf1 = rbf_kernel(hosing_ages, [[35]], gamma=0.1)
# این خط کرنل RBF را بین hosing_ages و مقدار ثابت [[35]] با گاما 0.1 محاسبه می‌کند.

rbf2 = rbf_kernel(hosing_ages, [[35]], gamma=0.2)

ax2 = ax.twinx()
# یعنی انگار داریم محور ایکس را تکرار میکنمی
# یعنی محور وای که ایجاد میشه روی محور ایکس قبلی اجرا میشه. یعنی محور ایکس بین دو تابع ax1 , ax2 مشترک است.

ax2.plot(hosing_ages, rbf1, color="black", lw=2)
ax2.plot(hosing_ages, rbf2, color="black", lw=2, ls="--")
# %% md
 #______________________________________________________________________________________________________
# ساخت یک مدل رگرسیون
"""
 به دو روش 
 که روش دوم خلاصه تر است
"""

# %% md
# روش 1
# %%
from sklearn.linear_model import LinearRegression

# %%
ss = StandardScaler()
target_values = ss.fit_transform(train_target.to_frame())
# to_frame تبدیل میکنه به دیتا فریم. روشی هم هست که دوبعدی بشه
# چون برای کار کردن با ابزارهای preprocessing که یکیش استانداد اسکیلر است دو بعدی بشه
target_values
# %%
model = LinearRegression()
model.fit(train_features_numeric[["median_income"]],
          target_values)
# %%
model.fit(train_features_numeric[["median_income"]],
          train_target.to_frame())
# %%
predictions = model.predict(train_features_numeric[["median_income"]].iloc[:5])

# %%
# نکته مهم : بعد از اینکه از مدلی که ساختیم را پریدیکشن کردیم حالا باید از بین صفر و یک در بیاریم

# برای اینکه مقادیر را که بین صفر و یک  ترنسفورم کرده بودیم را به حالت اول برگردانیم از اینورت ترسنفورم استفاده میکنیم
ss.inverse_transform(predictions)
# %% md
#  روش دوم:
"""
دیگه لازم نیست به صورت کد جداگانه ترنسفورم کنیم و بعد از پریدیکت دوباره اینورت ترنسفورم کنید
"""
# %%
from sklearn.compose import TransformedTargetRegressor

# %%
model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())

model.fit(train_features_numeric[["median_income"]],
          target_values)
# %%
model.predict(train_features_numeric[["median_income"]].iloc[:5])
# %% md
 #______________________________________________________________________________________________________
# ترنسفورمر کاستم یا سفارشی
"""ساخت ترنسفورمر دلخواه خودمون
ترنسفرم تابع را روی ستون اعمال میکنه 
بعدا یاد میگیریم که ترنسفرمری را طراحی کنیم که یک سری پارامتر را میخواد یاد بگیره 
"""
# %%
from sklearn.preprocessing import FunctionTransformer

# %%
logarithmic_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
# np.log تابع است
# np.log() به طور پیش‌فرض لگاریتم طبیعی (با پایه e)
# در پایتون، اگر می‌خواهید لگاریتم با پایه 10 را محاسبه کنید، باید صراحتاً از np.log10()
# اگر تابع شما اینورس داره بعد از اسم تابع باید اینورس را وارد کنید
# "اکسپونشیال" (Exponential) به طور کلی به معنای "نمایی" است # تابع np.exp در NumPy تابع نمایی با پایه e را محاسبه می‌کند.

logarithmic_transformer.transform(train_features_numeric[["population"]])
# %% md
# مثال از ساخت یک ترنسفورمر دیگر
"""
# ار بی اف را سفارشی میکنیم
# هدف معرفی kw_args  که برای دادن ورودی استفاده میشه
بیشتر بررسی کنم _ سوال دارم
"""
# %%
rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args={"Y": [[35.0]],
                                               "gamma": 0.1})
# %%
rbf_transformer.transform(train_features_numeric[["housing_median_age"]])
# %% md
 #______________________________________________________________________________________________________
# ترنسفورمر سفارشی یا توابع اماده روی چند ستون چطور کار میکنه
"""
سوال : توابع سفارشی با ار بی اف میتونه متفاوت باشه
"""
# %% md
 #______________________________________________________________________________________________________
# duck typing
# سایکیت لرن نحوه ساخت کلاس و چک کردن ابجکت را با اپروچ یا رویکرد duck typing انجام میده
# اگر چیزی شبیه اردک راه میره و شبیه اردک کوک کوک میکنه پس باید اردک باشه
# پس طبق داک تایپینگ درواقع ابجکت رو چک نمیکنه متد را چک میکنه
"""
#پس طبق داک تایپیگ برای ساخت یک ترنسفرم، باید کلاسی بنویسی که متد فیت، ترنسفرم، و فیت ترنسفرم را داشته باشه

ترنسفرم تابع را روی ستون اعمال میکنه 
بعدا یاد میگیریم که ترنسفرمری را طراحی کنیم که یک سری پارامتر را میخواد یاد بگیره 
"""
# %% md
"""
اگر  در سایکت لرن از یک سری بیس کلاسها استفاده کنیم میتونیم به متدهای آن دسترسی داشته باشیم.
مثلا :
TransformerMixin کمک میکنه که:
اگر فیت و ترنسفرمر را داشته باشیم به ما فیت ترنسفرمر را میده 

یا مثلا:
BaseEstimator
اگر از بیس استیمتور استفاده کنم انوقت بهم گت پارامز و ست پارامز را میده
get_params , set_params
"""
# %% md
 #______________________________________________________________________________________________________
# اسکیلر کاستوم یا سفارشی
"""ساخت اسکیلر دلخواه خودمون
"""
# %%
css = CustomStandardScaler()
css.fit_transform(train_features_numeric)
# %%
# ابزار مهم برای چک کردن اینکه ارایه مان اعضاش اعشاری باشه
# و ابزار مهم دیگر: چک کردن اینکه فیت شده یا نه
from sklearn.utils.validation import check_array, check_is_fitted

from sklearn.base import BaseEstimator, TransformerMixin


# %%
# سوال : بیشتر کار شود

# میخواهیم مثلا کلاس استاندارد اسکیلر خودمون را ایجاد کنیم
class CustomStandardScaler(BaseEstimator, TransformerMixin):
    # BaseEstimator باعث میشه که که پارامترهای دیانامیک را راحت بتواند بگیرد. و به راحتی متد گت پارامز و ست پارامز رو میده

    def __init__(self, hp=0):
        self.hp = hp

    # استیمیتورها به متد فیت و ترنسفرم احتیاج دارند پس دو تابع با یان نام مینویسیم
    def fit(self, X, y=None):
        # مقدار سلف و ایکس و ایگرگ را حتما میگیرد
        # اگر قرار باشد فقط روی فیچرها کار کند و y را نمیگیرد مقدارش را نان قرار میدیم

        X = check_array(X)
        # چک میکند که تمام ارایه اعشاری باشند

        # استاندار اسکیلر برای محاسبه از  میانگین و انحراف استانداد استفاده میکرد
        self.mean_of_X = X.mean(axis=0)
        # میانگین قرار روی سطرها گرفته شود پس محور را مساوی صفر قرار میدیم

        # انحراف استاندارد در تابع استانداد اسکیلر نیاز بود
        self.std_of_X = X.std(axis=0)

        # این هم باید حتما باید در متد فیت باشه
        self.n_features_in_ = X.shape[1]
        # کاربردش: برای اینکه چک کند تعداد فیچرهایی که در فیت میدی با انچه که به ترنسفرم میدی یکی باشه
        # نکته مهم: یک داده شده تا به تعداد ستونها باشه یعنی تعداد فیچرها، نه تعداد سمپل ها

        # ریترن سلف هم حتما باید نوشته بشه
        return self

    def transform(self, X):
        # فقط سلف و ایکس را میگیره

        check_is_fitted(self)
        # این چک میکنه که فیت شده یا نه

        assert self.n_features_in_ == X.shape[1]
        # بررسی کن که اگر تعداد فیچر برابر نبود ارور بده

        X = X - self.mean_of_X
        return X / self.std_of_X


# %%
css = CustomStandardScaler()
css.fit_transform(train_features_numeric)
# %% md

# %% md
 #______________________________________________________________________________________________________
# pipeline
"""
در یادگیری ماشین، "پایپ لاین" (Pipeline) مجموعه‌ای از مراحل پردازش داده است که به ترتیب اجرا می‌شوند. این مراحل معمولاً شامل موارد زیر هستند:

    پیش پردازش داده‌ها: این مرحله شامل تمیز کردن داده‌ها، تبدیل آنها به فرمت مناسب و استخراج ویژگی‌های مهم است.
    آموزش مدل: در این مرحله، مدل یادگیری ماشین با استفاده از داده‌های پیش پردازش شده آموزش داده می‌شود.
    ارزیابی مدل: در این مرحله، عملکرد مدل با استفاده از داده‌های آزمایشی ارزیابی می‌شود.
    استقرار مدل: در این مرحله، مدل آموزش داده شده برای استفاده در دنیای واقعی مستقر می‌شود.

مزایای استفاده از پایپ لاین:

    ساده‌سازی فرآیند: پایپ لاین‌ها فرآیند پیچیده یادگیری ماشین را به مراحل ساده‌تر و قابل مدیریت‌تر تقسیم می‌کنند.
    افزایش کارایی: پایپ لاین‌ها با خودکارسازی مراحل پردازش داده، کارایی فرآیند یادگیری ماشین را افزایش می‌دهند.
    جلوگیری از خطا: پایپ لاین‌ها با تعریف مراحل مشخص و قابل تکرار، از بروز خطا در فرآیند یادگیری ماشین جلوگیری می‌کنند.
    قابلیت استفاده مجدد: پایپ لاین‌ها را می‌توان برای پروژه‌های مختلف یادگیری ماشین استفاده کرد
"""
# %%
from sklearn.pipeline import Pipeline, make_pipeline

# %%
# روش 1:
numerical_features_pipeline = Pipeline([
    ("medianImputer", SimpleImputer(strategy="median")),
    ("minMaxScaler", MinMaxScaler())
])
# "medianImputer" مثلا برای ترنسفرمر اسم تعیین کردین
# هر چیزی غیر از آخری باید متد فت ترنسفرم را داشته باشه به جز آخری
# برای همین اخری میتونه مدل یادگیری ماشینتون باشه
# %%
# روش 2:
make_pipeline(
    SimpleImputer(strategy="median"),
    MinMaxScaler()
)

# این روش دیگه لازم به اسم گذاری نیست
# %%
numerical_features_pipeline.fit_transform(train_features_numeric)
# %% md
 #______________________________________________________________________________________________________
# ColumnTransformer
"""
ColumnTransformer یک ابزار پیش‌پردازش است که به شما امکان می‌دهد تبدیلات مختلفی را به ستون‌های مختلف داده‌ها اعمال کنید.

تصور کنید یک جدول داده دارید که شامل ستون‌هایی با انواع مختلف داده‌ها است (مثلاً اعداد، متن، دسته‌بندی). قبل از اینکه بتوانید این داده‌ها را به یک مدل یادگیری ماشین وارد کنید، باید آنها را به فرمتی تبدیل کنید که مدل بتواند آن را درک کند.

ColumnTransformer به شما امکان می‌دهد تا این کار را به صورت سازماندهی شده و کارآمد انجام دهید. به طور خاص:

    تبدیلات جداگانه: می‌توانید مشخص کنید که هر ستون باید با چه نوع تبدیلی پردازش شود. برای مثال، می‌توانید ستون‌های عددی را با StandardScaler مقیاس‌بندی کنید و ستون‌های متنی را با OneHotEncoder تبدیل کنید.
    مدیریت ستون‌های ناهمگن: به جای نوشتن کد پیچیده برای مدیریت هر ستون به صورت جداگانه، می‌توانید از ColumnTransformer برای تعریف تبدیلات به صورت یکجا استفاده کنید.


نحوه عملکرد:

ColumnTransformer لیستی از تاپل‌ها را می‌گیرد که هر تاپل شامل موارد زیر است:

    نام تبدیل: یک نام دلخواه برای تبدیل.
    تبدیل: یک شیء تبدیل scikit-learn (مانند StandardScaler، OneHotEncoder، یا یک تبدیل سفارشی).
    ستون‌ها: لیستی از نام ستون‌ها یا شاخص‌هایی که تبدیل باید روی آنها اعمال شود.

    نکته مهم: اگر بخواهیم  ستونی را نیاوریم به جای تبدیل فقط مینویسم:
    drop
    اگرچه وقتی نام ستونی را در داخل کالمن ترنسفرمر نیاوریم حذف میشه گویی دراپ کردیم.


    نکته مهم: اگر بخواهیم  عملی را ستونی انجام ندهیم به جای تبدیل فقط مینویسم:
    passthrough


هنگامی که ColumnTransformer اعمال می‌شود، هر تبدیل به ستون‌های مشخص شده اعمال می‌شود و نتایج به هم متصل می‌شوند تا یک آرایه یا DataFrame جدید ایجاد شود.

"""
# %%
from sklearn.compose import ColumnTransformer

# %%
train_features.columns
# %%
numerical_features = list(train_features.columns)
numerical_features.remove("ocean_proximity")
numerical_features
# %%
categorical_features = ["ocean_proximity"]
# %%
numerical_features_pipeline = Pipeline([
    ("medianImputer", SimpleImputer(strategy="median")),
    ("minMaxScaler", MinMaxScaler())
])
# %%
categorical_features_pipeline = Pipeline([
    ("mfImputer", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder())
])
# %%
total_transformation = ColumnTransformer([
    ("numerical", numerical_features_pipeline, numerical_features),
    ("categorical", categorical_features_pipeline, categorical_features)
])
#  کاری که در بالا انجام شد پایپ لاین ها را داخل کالمن ترنسفرمر را قرار داد

# %% md
 #______________________________________________________________________________________________________
# %% md

# %%
total_transformation.fit_transform(train_features)
# %%
total_transformation.get_feature_names_out()
# %% md
 #______________________________________________________________________________________________________
# all of the transformations
"""یک جمع بندی از ابتدای همین فایل تا به اینجا
"""
# %%
####  الف. روی نمونه خودمان روی ستون مدین اینکام طبقه بندی انجام دهیم
df["median_income_categories"] = pd.cut(df["median_income"],
                                        bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],
                                        labels=[1, 2, 3, 4, 5])

# ب. جدا کردن تست و ترین در نمونه  را طبق کتگوری ای که ایجاد کردیم انجام میدهیم
str_train_set, str_test_set = train_test_split(df,
                                               test_size=0.2,
                                               stratify=df["median_income_categories"],
                                               random_state=40)

# ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم
str_train_set = str_train_set.drop("median_income_categories", axis=1)
# axis=1 یعنی ستون را در نظر بگیر
str_test_set = str_test_set.drop("median_income_categories", axis=1)
# %%
train_data, test_data = train_test_split(df,
                                         test_size=0.2,
                                         stratify=df["median_income_categories"],
                                         random_state=40)
# %%
train_data = train_data.drop(columns="median_income_categories")
test_data = test_data.drop(columns="median_income_categories")
# %%
train_features = train_data.drop(columns="median_house_value")
train_target = train_data["median_house_value"]

test_features = test_data.drop(columns="median_house_value")
test_target = test_data["median_house_value"]
# %%
# train_features
# %%
heavy_tail_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler()
)

# نتیجه: یک پایپ لاین برای هوی تیل ها در نظر میگیریم
# %%
categorical_features_pipeline = Pipeline([
    ("mfImputer", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder())
])
# نتیجه: یه پایپ لاین برای ستون غیر عددی
# %%
housing_median_age_rbf_transformer = FunctionTransformer(rbf_kernel,
                                                         feature_names_out="one-to-one",
                                                         kw_args={"Y": [[35.0]], "gamma": 0.1})
# %%
default_numerical_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler()
)


# نتیجه: برای دیفالت ریماندر کالمن ترنسفورمر هست. یعنی اگر ستونی را در کالمن ترنسفورمر نداریم حذفشون نکنه و عملیات بالا را قرار بده
# %%
# هدف: این تابع برای محاسبه نسبت بین مقادیر ستون اول و ستون دوم یک آرایه NumPy طراحی شده است.
def ratio_of_columns(X):
    return X[:, [0]] / X[:, [1]]
    # همه سطرها و ستون صفر را تقسیم کند بر همه سطرها از ستون یک
    # توجه داشته باشید که [0] در داخل براکت‌های دیگر قرار داده شده است، که باعث می‌شود نتیجه یک آرایه دو بعدی باشد.


# هدف: این تابع برای تعیین نام ستون خروجی FunctionTransformer طراحی شده است.
def ratio_of_columns_name(function_transformer, feature_names_in):
    return ["ratio"]
    # function_transformer: این پارامتر یک شیء FunctionTransformer است
    # feature_names_in: این پارامتر نام ستون های ورودی است.
    # return ["ratio"]: این قسمت از کد یک لیست حاوی یک رشته ("ratio") را برمی‌گرداند. این رشته به عنوان نام ستون خروجی FunctionTransformer استفاده می‌شود.
    # نتیجه: این تابع همیشه لیست ["ratio"] را برمیگرداند.


def ratio_of_columns_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        # این مرحله مقادیر گمشده را در داده‌ها با میانه ستون مربوطه پر می‌کند.

        FunctionTransformer(ratio_of_columns,
                            feature_names_out=ratio_of_columns_name),
        # ratio_of_columns_name برای تعیین نام ستون خروجی استفاده می‌کند
        StandardScaler()
        # این مرحله داده‌ها را به گونه‌ای مقیاس‌بندی می‌کند که میانگین آنها صفر و انحراف معیار آنها یک شود.

    )


# نتیجه: یه پایپ لاین برای ستونهایی که بر هم تقیسم میشه درست کردیم
# %%
all_transformations = ColumnTransformer([
    ("bedrooms", ratio_of_columns_pipeline(), ["total_bedrooms", "total_rooms"]),
    ("rooms_per_house", ratio_of_columns_pipeline(), ["total_rooms", "households"]),
    ("people_per_house", ratio_of_columns_pipeline(), ["population", "households"]),
    ("log", heavy_tail_pipeline, ["total_bedrooms", "total_rooms",
                                  "population", "households", "median_income"]),
    ("rbf", housing_median_age_rbf_transformer, ["housing_median_age"]),
    ("cat", categorical_features_pipeline, ["ocean_proximity"])

],
    remainder=default_numerical_pipeline, force_int_remainder_cols=False)
# در نسخه‌های فعلی scikit-learn، ستون‌های باقی‌مانده به عنوان شاخص‌های عددی (int) ذخیره می‌شوند.
# در نسخه‌های آینده، این ستون‌ها به عنوان نام ستون‌ها (str) ذخیره خواهند شد.
# برای جلوگیری از این اخطار و استفاده از رفتار جدید،
# می‌توانید هنگام ایجاد ColumnTransformer، پارامتر force_int_remainder_cols را به False تنظیم کنید.

# ورودی های کالمن ترنسفرم ای بود که ترنسفرم ها را وارد کنیم و یک ریمایندر
# که ریمایندر یعنی اگر ترنسفرمری برای ستونی ندادی به طوری پیشفرض ان ستون حذف میشه ولی ما در اینجا مشخص کردیم که چیکار کنه
# %%
train_features_transformed = all_transformations.fit_transform(train_features)
# درسته که این کالمن ترنسفرم از ما فقط ترین فیچر را میخواد
# اما زمانی که مدل را روی این کالمن ترنسفرمر فیت میکنیم ترین تارگت را هم میخواد
# %%
train_features_transformed.shape
# %%
all_transformations.get_feature_names_out()
# %%
train_features_transformed
# %% md

# %% md
 #______________________________________________________________________________________________________
# شروع مدلهای یادیگری ماشین
# %% md
 #______________________________________________________________________________________________________
# LinearRegression
# %%
from sklearn.linear_model import LinearRegression

# %%
linear_regression_pipeline = make_pipeline(all_transformations,
                                           LinearRegression())
# در پایپ لاینی که درست میکنیم اسم مدل باید اخر نوشته شود چون میخواهیم روشش پریدیکشن داشته باشیم
# %%


linear_regression_pipeline.fit(train_features,
                               train_target)
# مدل را با استفاده از fit آموزش دادیم.

# درسته که این پاپ لاین از ما فقط ترین فیچر را میخواست
# اما زمانی که میخواهیم مدل را روی این کالمن ترنسفرمر فیت کنیم ترین تارگت را هم میخواد
# %%
house_value_prediction = linear_regression_pipeline.predict(train_features)
# %%
house_value_prediction
# مقادیری که مدل پیش بینی کرده
# %%
train_target.values
# اصل مقادیری که بوده
# %% md
# ارزیابی مدل رگرسیون
# %%
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# %%
# mean_squared_error(train_target.values, house_value_prediction,
#                    squared=False)
# سوال : برای من با وجودی که نسخه سایکیت لرن 1.6.1 بود خطای میداد
# %%
mean_absolute_error(train_target.values, house_value_prediction)
# %%
r2_score(train_target.values, house_value_prediction)
# %% md
# DecisionTree
درخت
تصمیم
# %%
from sklearn.tree import DecisionTreeRegressor

# %%
dt_regression_pipeline = make_pipeline(all_transformations,
                                       DecisionTreeRegressor())
# %%
dt_regression_pipeline.fit(train_features,
                           train_target)
# %%
mean_absolute_error(train_target.values,
                    dt_regression_pipeline.predict(train_features))

# جوابی که میده انقد دقیق که میگوییم اورفیت شده و مدل بسیار پیچیده است
# مدل قبل هم که رگرسیون بود هم انقد خطا زیاد بود که فهمیدیم آندر فیت است و زورش نمیرسید
# %%
r2_score(train_target.values,
         dt_regression_pipeline.predict(train_features))
# %% md
 #______________________________________________________________________________________________________
# Cross Validation
# ولیدنشن
#### بخشی را از دادهای ترین را به عنوان ولیدیشن جدا میکنیم که البته جدای داده های تست هستش
##### برای بررسی و تست، قبل از تست اصلی هستش
# %%
from sklearn.model_selection import cross_val_score

# %%
dt_mae = -cross_val_score(dt_regression_pipeline,
                          train_features,
                          train_target,
                          scoring="neg_mean_absolute_error",
                          cv=5)
# اولین عضوش مدل را میگذاریم
# دومین عضوش ایکس و بعدش وای را وارد میکنیم
# scoring یعنی با چه معیاری کار کن.
# سی وی cv یعنی به تعداد باری که تعیین میکنی که داده را مثلا 5 قسمت مساوی میکند و یه یک پنجم را به عنوان ولیدیشن برمیداره

dt_mae
# که 5 تا مدل بهمون میده
# %%
pd.DataFrame(dt_mae).describe()
# %% md

# GridSearchCV
#### بهتر است کراس ولیدیشن قبلی است
# %%
from sklearn.model_selection import GridSearchCV

# که پارامتر اول میتونه مدل یا پایپ لاینمون باشه
# پارامتر دوم پارام گرید هست یعنی فضایی که قرار است داخلش جستجو را انجام دهد را میخواد
# پارامتر بعدی اسکورینگ
# سی وی هم داره یعنی کراس ولیدیشن هم میده در اخر بهترین را میده
# %% md
# method1
# روش اول
# %%

# %%
param_grid = {"max_depth": [5, 10, 15]}
# بهتره به صورت دیشکنری نوشته شود
# %%
gs = GridSearchCV(DecisionTreeRegressor(),
                  param_grid,
                  scoring="neg_mean_absolute_error",
                  cv=5)

# داخل پرانتز مدل اگر پارامتری را بگذاریم یعنی میخواهیم اون پاراتر ثابت باشه
# %%
gs.fit(train_features_transformed, train_target)
# %%
gs.best_params_
# %% md
# method2
# GridSearchCV
# روش دوم که بهتره
# عالی مهم
### به این روش حتی ابزارهای ترنسفورمشن هم اگر هایپر پارامتر داشته باشه انها را هم بهینه میکند
# %%
dt_regression_pipeline = Pipeline([
    ("transformation", all_transformations),
    ("dtr", DecisionTreeRegressor())
])
# %%
param_grid = {"transformation__rbf__kw_args": [{"Y": [[35.0]], "gamma": 0.1},
                                               {"Y": [[35.0]], "gamma": 0.2}],
              "dtr__max_depth": [5, 10, 15]}

# __rbf__kw_args که نوشته شده یکی از پارامترهای داخلی ترنسفورمرمون بود
# %%
gs = GridSearchCV(dt_regression_pipeline,
                  param_grid,
                  scoring="neg_mean_absolute_error",
                  cv=5)
# %%
gs.fit(train_features, train_target)
# %%
gs.best_params_
# %%
gs.cv_results_
# برای اینکه نتیجه سرچ هایی که انجام داده رو ببینیم
# که بهتره داخل یک دیتا فریمی بریزیم که در خط بعدی انجامش دادم
# %%
pd.DataFrame(gs.cv_results_).sort_values(by="mean_test_score",
                                         ascending=False)
# %% md
# method3
# RandomizedSearchCV
# روش سوم که برای فضای بزرگ مناسبه و نمیخواهیم خیلی دقیق بهترین را انتخاب کنیم
#### به صورت رندوم سرچ میکنه
### برای فضای بزرگ مناسبه چون روش قبلی ممکنه خیلی بزرگ بشه
"""
یعنی تمام حالاتی که ممکنه رو بررسی نکن مثلا 10 تا ترکیب را به صورت رندوم انتخاب کن و بهترین حالت رو بده
 که این مقدار که اینجا 10 است  پارامتر ایتریشن است
"""
# %%
from scipy.stats import randint
from sklearn.model_selection import RandomizedSearchCV

# %%
dt_regression_pipeline = Pipeline([
    ("transformation", all_transformations),
    ("dtr", DecisionTreeRegressor())
])

# تعریف توزیع‌های احتمالی برای پارامترها
param_distributions = {
    "dtr__max_depth": randint(5, 100),  # توزیع اعداد تصادفی بین 5 و 99 (100 شامل نمی‌شود)
    "dtr__min_samples_split": randint(2, 20),
    "dtr__min_samples_leaf": randint(1, 10),
    "transformation__rbf__kw_args": [
        {"Y": [[35.0]], "gamma": 0.1},
        {"Y": [[35.0]], "gamma": 0.2},
        {"Y": [[35.0]], "gamma": 0.3},
        {"Y": [[35.0]], "gamma": 0.4}
    ]
}

# ایجاد و اجرای RandomizedSearchCV
rs = RandomizedSearchCV(dt_regression_pipeline,
                        param_distributions,
                        n_iter=10,  # تعداد تکرارها (تعداد ترکیب‌های پارامترها)
                        scoring="neg_mean_absolute_error",
                        cv=5,
                        random_state=42)  # برای تکرارپذیری

# فرض بر این است که train_features و train_target قبلاً تعریف شده‌اند
rs.fit(train_features, train_target)

# نمایش بهترین پارامترها
print(rs.best_params_)

# نمایش بهترین امتیاز
print(rs.best_score_)
# %%
# حالا با توجه به اینکه بهترین پارامترها را توسط GridSearchCV پیدا کردیم یا RandomizedSearchCV
# بهترین مدل را با استیمیتور میبینیم که چی هست
# مهم و عالی که در زیر به صورت زیپ نشون میدیم که متوجه اهمیتش میشیم
housing_model = gs.best_estimator_
housing_model["dtr"]
# %%
housing_model["dtr"].feature_importances_.round(3)
# این خط کد میگوید برای این مدل اهمیت فیچرهایی که داشتیم چقدر بود
# round(3) تا سه رقم اعشار رندش کن
# %%
sorted(zip(housing_model["dtr"].feature_importances_.round(3),
           housing_model["transformation"].get_feature_names_out()),
       reverse=True)
# اگر در خروجی دقت کنید مثلا np.float64(0.0) یعنی این ویژگی برای مدل اهمیتی نداشته
# اما مثلا  (np.float64(0.055), 'log__median_income'), با توجه به عدد 0.055 یعنی با اهمیت بوده
# %%
# ارزیابی مدل روی تست
mean_squared_error(test_target,
                   housing_model.predict(test_features),
                   squared=False)

# سوال : مهم  برای squared ارور میده در حالی که نسخه سایکیت لرنم پایین نیست
# %% md
 #_________________________________________________________________________________________________________________________________________
# "فاصله اطمینان" (Confidence Interval)
"""
در سایکیت-لِرن (scikit-learn)، مفهوم "فاصله اطمینان" (Confidence Interval) به طور مستقیم برای تمام مدل‌های یادگیری ماشین پیاده‌سازی نشده است.
 با این حال، روش‌هایی وجود دارند که می‌توانید با استفاده از آن‌ها، فواصل اطمینان را برای پیش‌بینی‌های مدل‌های سایکیت-لِرن محاسبه کنید.

تعریف فاصله اطمینان:

فاصله اطمینان، محدوده‌ای از مقادیر است که با احتمال مشخصی، مقدار واقعی یک پارامتر یا پیش‌بینی در آن قرار می‌گیرد.
 به عنوان مثال، یک فاصله اطمینان 95% نشان می‌دهد که اگر آزمایش را بارها تکرار کنیم، 95% از فواصل اطمینان محاسبه شده، مقدار واقعی را در بر خواهند داشت.

روش‌های محاسبه فاصله اطمینان در سایکیت-لِرن:

    روش بوت‌استرپ (Bootstrap):
        این روش یک روش غیرپارامتری است که با نمونه‌گیری مجدد از داده‌های اصلی، توزیع تخمین زده شده را برای پارامتر یا پیش‌بینی محاسبه می‌کند.
        سپس، از این توزیع برای محاسبه فاصله اطمینان استفاده می‌شود.
        سایکیت-لِرن به طور مستقیم تابع بوت‌استرپ را ارائه نمی‌دهد، اما می‌توانید از کتابخانه‌های دیگری مانند scipy.stats یا statsmodels برای پیاده‌سازی آن استفاده کنید.

    روش‌های پارامتری (برای مدل‌های خاص):
        برخی مدل‌های سایکیت-لِرن، مانند رگرسیون خطی، روش‌های پارامتری برای محاسبه فاصله اطمینان ارائه می‌دهند.
        در این روش‌ها، با استفاده از توزیع‌های آماری شناخته شده (مانند توزیع t)، فاصله اطمینان محاسبه می‌شود.
        به عنوان مثال، در رگرسیون خطی، می‌توانید با استفاده از تابع statsmodels.OLS، فواصل اطمینان را برای ضرایب رگرسیون محاسبه کنید.

    روش‌های مبتنی بر خطای باقی‌مانده (Residual-based):
        این روش‌ها با استفاده از خطاهای باقی‌مانده (تفاوت بین مقادیر واقعی و پیش‌بینی شده)، توزیع خطا را تخمین می‌زنند و سپس از این توزیع برای محاسبه فاصله اطمینان استفاده می‌کنند.
        این روش‌ها معمولاً برای مدل‌های رگرسیون استفاده می‌شوند.
"""
# %%
from scipy import stats

# %%
ci = 0.95
# %%
se = (test_target.values - housing_model.predict(test_features)) ** 2
# %%
np.sqrt(stats.t.interval(ci,
                         len(se) - 1,
                         loc=np.mean(se),
                         scale=stats.sem(se)))
# %% md
 #_________________________________________________________________________________________________________________________________________
# نهایی کردن مدل
"""
موارد استفاده از joblib:

   -ذخیره مدل‌های یادگیری ماشین برای استفاده مجدد در برنامه‌های دیگر.
   -به اشتراک‌گذاری مدل‌های آموزش‌داده‌شده با دیگران.
   -ذخیره و بارگیری داده‌های بزرگ به طور کارآمد.
   -اجرای پردازش های موازی.

فایل‌های .pkl چیستند؟

   فایل‌های .pkl فایل‌هایی هستند که با استفاده از ماژول pickle یا کتابخانه joblib در پایتون ایجاد می‌شوند.
   این نوع فایل ها برای ذخیره سازی باینری در پایتون استفاده می شوند.
   آنها حاوی سریال‌سازی‌شده اشیاء پایتون هستند، به این معنی که اشیاء پایتون به یک قالب قابل ذخیره و بازیابی تبدیل می‌شوند.
   به طور معمول، از فایل‌های .pkl برای ذخیره مدل‌های یادگیری ماشین، داده‌های بزرگ و سایر اشیاء پایتون استفاده می‌شود
"""
# %%
import joblib
#
# # %%
# joblib.dump(housing_model, "housing_model.pkl")
# # %%
# housing_model_loaded = joblib.load("housing_model.pkl")
# # %%
# housing_model_loaded
# %%

# %% md
 #_________________________________________________________________________________________________________________________________________
### ML operations (MLOps)