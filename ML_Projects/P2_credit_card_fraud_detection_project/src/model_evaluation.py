# ูุฑุจูุท ุจู ฺฏุงู 8 ู 9
# src/model_evaluation.py
import os
import pandas as pd
from sklearn.model_selection import cross_validate
from sklearn.metrics import (
    make_scorer, accuracy_score, f1_score,
    precision_score, recall_score, roc_auc_score, roc_curve, auc
)
import matplotlib.pyplot as plt
import seaborn as sns

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 1)  ุชุงุจุน ุงุตู ุงุฑุฒุงุจ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def evaluate_models_cv(models, X, y, output_dir="outputs", cv=5):
    """
    Cross-validation ุฑู ุชูุงู ูุฏูโูุงุ ุฐุฎุฑู ูุชุฑฺฉโูุง ู ุฑุณู ูููุฏุงุฑ ููุงุณูโุง.
    """
    os.makedirs(output_dir, exist_ok=True)

    scorers = {
        "accuracy":  make_scorer(accuracy_score),
        "f1":        make_scorer(f1_score),
        "precision": make_scorer(precision_score),
        "recall":    make_scorer(recall_score),
        "roc_auc":  "roc_auc"
    }
    # ุจุง ุงุณุชูุงุฏู ุงุฒ make_scorer ุฏุงุฑู:
    # ุจู ุชุงุจุน ุงูุชุงุฒ ุฏู (ูุงููุฏ accuracy_score ุprecision ุrecall ุ f1 ู.. ) ูุฑูุฏ ูุง y_true ู y_pred  (ุฑุง ฺฉู ูุงุฒ ุฏุงุฑุฏ ) ุฑุง ู ุฏูู ู
    # ุขูุฑุง ุจู ูุฑูุช ุงุณุชุงูุฏุงุฏ ุชุจุฏู ูฺฉูู ุชุง ุฏุฑ ุชูุงุจุน ุงุฑุฒุงุจ ูุชูุงุจู (cross-validation) ูุงููุฏ cross_validate ูุงุจู ุงุณุชูุงุฏู ุจุงุดุฏ.
    # ูฺฉุชู2: ุฏุฑ scorers ููุฏุงุฑ ุงฺฉูุฑุณ ู F1 ู... ูุญุงุณุจู ููโุดูุฏ ุตุฑูุงู "ุชุนุฑู ูโฺฉูุฏ"

    results = []
    for name, model in models.items():
        print(f"๐ ุงุฑุฒุงุจ ูุฏู: {name}")

        # ูพุงุงุฑูุชุฑscoring
        scores = cross_validate(model, X, y, cv=cv, scoring=scorers, n_jobs=-1)
        # scores = cross_validate(...): ุงู ุฎุทุ ุชูุงู ูุฑุขูุฏ Cross-validation ุฑุง ุจุฑุง ฺฉ ูุฏู ุฎุงุต (model) ุงูุฌุงู ูโุฏูุฏ.
        # ุนู ุงฺฏุฑ cv=5 ุจุงุดุฏุ cross_validate ูุฏู ุฑุง 5 ุจุงุฑ ุขููุฒุด ุฏุงุฏู ู ุงุฑุฒุงุจ ูโฺฉูุฏ.
        # ฺฉู ุฏุฑ ูุฑ ูุฑุญูู ูุฑ ฺฉ ุงุฒ scorers ุฑุง ูู ุจุฑุง ูุฑ ูุฏู ูุญุงุณุจู ูฺฉูุฏ
        # ุฏุฑ ูพุงุงู ุงู ุฎุทุ ูุชุบุฑ scores ฺฉ ุฏฺฉุดูุฑ ุญุงู ุชูุงู ูุชุงุฌ (ูุซูุงู 5 ููุฏุงุฑ ุจุฑุง ุฏูุชุ 5 ููุฏุงุฑ ุจุฑุง F1 Score ู...) ุงุฒ ุงู 5 ุชฺฉุฑุงุฑ ุฎูุงูุฏ ุจูุฏ.
        # ุงู ูุฑุญูู ุชุง ุฒูุงู ฺฉู scores ุจู ุทูุฑ ฺฉุงูู ูพุฑ ูุดุฏู ุจุงุดุฏ (ุนู 5 ุชฺฉุฑุงุฑ ุงูุฌุงู ูุดุฏู ุจุงุดุฏ)ุ ุจู ุฎุท ุจุนุฏ ููโุฑูุฏ.
        # ูฺฉุชู : ูพุงุฑุงูุชุฑ n_jobs ุฏุฑ ุชุงุจุน cross_validate (ู ุจุณุงุฑ ุฏฺฏุฑ ุงุฒ ุชูุงุจุน Scikit-learn ฺฉู ุงุฒ ูพุฑุฏุงุฒุด ููุงุฒ ูพุดุชุจุงู ูโฺฉููุฏ) ุจุฑุง ฺฉูุชุฑู ุชุนุฏุงุฏ ูุณุชูโูุง CPU ุง "job" ูุง ุงุณุชูุงุฏู ูุดู ฺฉู ุจู ุตูุฑุช ููุงุฒ ุจุฑุง ุงุฌุฑุง ุนููุงุชโูุง ุจู ฺฉุงุฑ ฺฏุฑูุชู ูุดู.
        #
        # ููุฏุงุฑ    n_jobs = -1: ุงู ููุฏุงุฑ ูฺู ุจู cross_validate ูฺฏู ฺฉู "ุชูุงู ูุณุชูโูุง CPU ููุฌูุฏ ุฑู ุงุณุชูุงุฏู ฺฉู". ุงู ูุนูููุงู ุจูุชุฑู ฺฏุฒูู ุจุฑุง ุณุฑุนโุชุฑ ฺฉุฑุฏู ูุฑุขูุฏ cross-validationุ ุจู ุฎุตูุต ุฏุฑ ูุฌููุนู ุฏุงุฏูโูุง ุจุฒุฑฺฏ ุง ููุช cv (ุชุนุฏุงุฏ fold ูุง) ุจุงูุง ุจุงุดูุ ูุณุช.
        #   ููุฏุงุฑ  n_jobs = 1: ุจู ุงู ูุนูู ฺฉู ููุท ฺฉ ูุณุชู CPU ุงุณุชูุงุฏู ูุดู (ุงุฌุฑุง ุณุฑุงู).
        #    ููุฏุงุฑ n_jobs = N (ูุซูุงู 2 ุง 4): ุจู ุงู ูุนูู ฺฉู N ูุณุชู CPU ุจู ุตูุฑุช ููุงุฒ ฺฉุงุฑ ูโฺฉููุฏ.

        res = {
            "Model":     name,
            "Accuracy":  scores["test_accuracy"].mean(),
            "F1 Score":  scores["test_f1"].mean(),
            "Precision": scores["test_precision"].mean(),
            "Recall":    scores["test_recall"].mean(),
            "ROC AUC":   scores["test_roc_auc"].mean()
        }
        # res = { ... }: ูพุณ ุงุฒ ุงูฺฉู scores ุจู ุทูุฑ ฺฉุงูู ุขูุงุฏู ุดุฏุ ุงู ุฎุท ุงุฌุฑุง ูโุดูุฏ.
        # ุฏุฑ ุงู ูุฑุญููุ ููุงุฏุฑ ูุงูฺฏูโฺฏุฑูุชู ุดุฏู (ูุซู scores["test_accuracy"].mean()) ุงุฒ ุขุฑุงูโูุง ููุฌูุฏ ุฏุฑ scores ูุญุงุณุจู ุดุฏู
        # ู ุฏุฑ ุฏฺฉุดูุฑ res ุฐุฎุฑู ูโุดููุฏ.
        #  ููู: ููุช ุฏุฑ ูพุงุฑุงูุชุฑ scoring ุจู cross_validate ฺฉ ุฏฺฉุดูุฑ ูโุฏูู ุชุงุจุน cross_validate ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ ุงุณู ุชุณุช ุฑุง ุงุจุชุฏุงุดุงู ุงุถุงูู ูฺฉูู

        for k, v in res.items():
            if k != "Model":
                print(f" {k}: {v:.4f}")
        results.append(res)
        #ุญุงูุง ฺฉู ุฏฺฉุดูุฑ res ุจุง ููุงุฏุฑ ูุงูฺฏู ููุง (ฺฉู ูุฑ ฺฉุฏุงู ฺฉ ุนุฏุฏ ุชฺฉ ูุณุชูุฏ) ูพุฑ ุดุฏู ุงุณุชุ ุงู ุญููู for ุงุฌุฑุง ูโุดูุฏ.
        #ุงู ุญููู ุฑู ุขุชูโูุง ุฏฺฉุดูุฑ res ูพูุงุด ูโฺฉูุฏ ู
        # ูุฑ ูุนุงุฑ (ูุซู Accuracyุ F1 Score ู...) ุฑุง ุจู ููุฑุงู ููุฏุงุฑ ูุงูฺฏู ุขู (ุจุง 4 ุฑูู ุงุนุดุงุฑ) ฺุงูพ ูโฺฉูุฏ.

    # DataFrame ู ุฐุฎุฑู CSV
    df = pd.DataFrame(results)
    csv_path = os.path.join(output_dir, "cv_metrics.csv")
    df.to_csv(csv_path, index=False)
    print(f"โ CV metrics saved: {csv_path}")

    # ูููุฏุงุฑ ููุงุณูโุง
    plot_metrics_comparison(df, output_dir)
    # ุงูฺฉู ุชุงุจุน ุจุนุฏ ุฑุง ุฏุงุฎู ุงู ุชุงุจุน ุตุฏุง ูุฒูู:
    # ุงฺฏุฑ plot_metrics_comparison(df, output_dir) ุฑุง ูุณุชููุงู ุฒุฑ ุชุนุฑู ุชุงุจุน plot_metrics_comparison ูโููุดุชูุ
    # ุขู ุฎุท ุจูุงูุงุตูู ูพุณ ุงุฒ ุชุนุฑู ุชุงุจุน ุงุฌุฑุง ูโุดุฏ (ุจู ูุญุถ ุงูฺฉู ูุงู ูพุงุชูู ุจุงุฑฺฏุฐุงุฑ ุดูุฏ)
    # ู ูู ุฒูุงู ฺฉู ุดูุง evaluate_models_cv ุฑุง ูุฑุงุฎูุงู ูโฺฉูุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ:
    #     ฺฉุฏ ูุงููุธู ุดูุฏ: ุชูุงุจุน ุจุฏูู ูฺ ูุฑุงุฎูุงู ุตุฑุญุ ุจู ูุญุถ ุจุงุฑฺฏุฐุงุฑ ูุงู ุงุฌุฑุง ุดููุฏ.
    #     ูุงุจุณุชฺฏ ุจู ูุชุบุฑูุง ุฎุงุฑุฌ ุงุฒ scope: df ู output_dir ุฏุฑ ุขู ุฒูุงู ูููุฒ ุชุนุฑู ูุดุฏูโุงูุฏ ู ุจุง ุฎุทุง NameError ููุงุฌู ูโุดุฏู.

    return results

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 2)  ุชูุงุจุน ฺฉูฺฉ ุฑุณู ูููุฏุงุฑูุง
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def plot_metrics_comparison(df, output_dir="outputs"):
    """Bar-chart ููุงุณู Accuracy / F1 / ROC-AUC ุจู ูุฏูโูุง ุฑุง ุฐุฎุฑู ูโฺฉูุฏ."""
    os.makedirs(output_dir, exist_ok=True)
    plt.figure(figsize=(10, 6))

    # ุชุจุฏู ุณุชูู ูุงุงุฒ ุญุงูุช "wide" ุจู "long"
    df_melted = df.melt(id_vars="Model",
                        value_vars=["Accuracy", "F1 Score", "ROC AUC"],
                        var_name="Metric",
                        value_name="Score")
    # ุชุงุจุน melt ุฏุชุงูุฑู ุฑุง ุงุฒ ุญุงูุช ูพูู (wide) ุจู ุญุงูุช ุจุงุฑฺฉ (long) ุชุจุฏู ูโฺฉูุฏ ุชุง ุจุชูุงู ุจุง Seaborn ูููุฏุงุฑ ุฑุณู ฺฉุฑุฏ.
    # ุญุงูุช ูพูู ุง wide ุนู ูุฑ ูุชุฑฺฉุ ฺฉ ุณุชูู ุฌุฏุงุณุช ฺูู ุงุทูุงุนุงุช ุฏุฑ ุณุชููโูุง ูุฎุชูู ูพูู ุดุฏู.
    #  ุงูุง ููุช ุชูุงู ูุชุฑฺฉ ูุง ุฑุง ุฏุฑ ฺฉ ุณุชูู ุฑุฏู ฺฉูู ูุฑูุช Long Format ุง ุจุงุฑฺฉ ุฑุง ุฎูุงูู ุฏุงุดุช.
    # var_name ู value_name ููุท ูุงู ุณุชูู ูุณุชูุฏุ ุจูฺฉู ุณุชููโูุง ูุณุชูุฏ ฺฉู ุฏุงุฏูโูุง ุฑุง ูู ุฏุฑ ุฎูุฏ ูฺฏู ูโุฏุงุฑูุฏ.
    #  ูพุงุฑุงูุชุฑ id_vars:
    # ุณุชูู ููุช ุงุณุช ู ฺุฒุณุช ฺฉู ููโุฎูุง ุชุบุฑ ฺฉูู (ูุซูุงู ูุงู ูุฏูโูุง ูุซู Logistic)
    #
    # - ูพุงุฑุงูุชุฑ value_vars:
    # "ุณุชููโูุง ฺฉู ูโุฎูุง ุจุงุฑฺฉ ฺฉู"
    # ูุซูุงู Accuracy, F1 Score, ROC AUC
    #
    # - ูพุงุฑุงูุชุฑ  var_name :
    # "ุงุณู ุณุชูู ุฌุฏุฏ ุจุฑุง ุงุณูโูุง ูุฏู"
    # ูุซูุงู Metric, ฺฉู ุชูุด ูุงุฏ: Accuracy, F1 Score, ...
    # ุจู ุนุจุงุฑุช ุฏฺฏุฑvar_name ูุงู ุณุชูู ุงุณุช ฺฉู ุนููุงู ุณุชููโูุง value_vars ุฑุง ุฏุฑ ุฎูุฏ ูฺฏู ูโุฏุงุฑุฏ.
    #
    # - ูพุงุฑุงูุชุฑ value_name:
    # " ุณุชูู ุฌุฏุฏ ุจุฑุง ููุงุฏุฑ".
    # ูุซูุงู Score, ฺฉู ุชูุด ูุงุฏ: 0.85, 0.88, ...
    # value_name ูุงู ุณุชูู ุงุณุช ฺฉู ููุงุฏุฑ ุนุฏุฏ ูุฑุจูุท ุจู ุขู ุนููุงูโูุง ุฑุง ุฏุฑ ุฎูุฏ ูฺฏู ูโุฏุงุฑุฏ.

    sns.barplot(data=df_melted, x="Model", y="Score", hue="Metric")
    # hue="Metric" ุนู ุจู ุงุฒุง ูุชุฑฺฉ ูุง ุฑูฺฏ ุฏุฑ ูุธุฑ ุจฺฏุฑู
    #  ูฺฉุชู: ุจุฑุชุฑ ุณุจูุฑู ุฏุฑ ุงูุฌุง ูุณุจุช ุจู ุจุงุฑฺุงุฑุช ูุช ูพูุงุช ูุจ:
    # ููุงูุทูุฑฺฉู ูุจูู DataFrame ุฑุง ุจู ูุฑูุช "long" ุง df_melted ุชุจุฏู ฺฉุฑุฏู.
    # Seaborn ุจู ุทูุฑ ุทุจุน ุจุฑุง ฺฉุงุฑ ุจุง ุฏุงุฏูโูุง "long format" ุทุฑุงุญ ุดุฏู ุงุณุช.

    # ุจุง ฺฉ ุฎุท ฺฉุฏ ุณุงุฏู Seaborn ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ ฺฉุงุฑูุง ุฒุฑ ุฑุง ุงูุฌุงู ูุฏู:
    # 1. ฺฏุฑููโุจูุฏ (ุจุฑ ุงุณุงุณ Model ู Metric)ุ
    # ุงูู.  ูุญูุฑ x (Model): Seaborn ููุงุฏุฑ ููุญุตุฑ ุจู ูุฑุฏ ุณุชูู "Model" (ูุซูุงู "Logistic Regression", "Decision Tree" ู ...) ุฑู ุดูุงุณุง ูโฺฉูู
    # ู ุจุฑุง ูุฑ ฺฉุฏูู ฺฉ ฺฏุฑูู ุงุตู ุฑู ูุญูุฑ X ุงุฌุงุฏ ูโฺฉูู.
    # ุจ. ูพุงุฑุงูุชุฑ hue (Metric): ุงูุฌุง ุฌุงุฏู ฺฏุฑููโุจูุฏ Seaborn ููุงุงู ูุดู.
    # hue="Metric" ุจู Seaborn ูฺฏู ฺฉู ุฏุฑูู ูุฑ ฺฏุฑูู ุงุตูู Modelุ ุฒุฑฺฏุฑููโูุง ุจุฑ ุงุณุงุณ ููุงุฏุฑ ุณุชูู "Metric" (Accuracy, F1 Score, ROC AUC) ุงุฌุงุฏ ฺฉู.
    # 2.ูุญุงุณุจู ูุงูฺฏู (ุงฺฏุฑ ฺูุฏู ููุฏุงุฑ Score ุจุฑุง ฺฉ Model/Metric ูุฌูุฏ ุฏุงุดุช ุงูุง ุฏุฑ ุงูุฌุง ูุง ูุจูุง ูุงูฺฏุฑ ุฑุง ุงูุฌุงู ุฏุงุฏู)
    # 3.ู ุฑุณู ูููโูุง ุฑุง ุงูุฌุงู ูโุฏูุฏ.

    plt.ylim(0, 1)
    # ุงู ุฎุท ุชุนู ูโฺฉูุฏ ฺฉู ูุญูุฑ y ูููุฏุงุฑ ุงุฒ ููุฏุงุฑ ฐ ุชุง ฑ ุจุงุดุฏ.

    plt.title("๐ Metrics Comparison Between Models")

    plt.tight_layout()
    #  ุงู ุชุงุจุน ฺฉูฺฉ ูฺฉูุฏ ฺฉู: ุฌููฺฏุฑ ุงุฒ ูููพูุดุงู ุนูุงุตุฑ ูููุฏุงุฑ.
    # ุชูุธู ูุงุตูู ุจู ุฒุฑูููุฏุงุฑูุง.
    # ุชูุธู ุญุงุดูโูุง ุงุทุฑุงู ูููุฏุงุฑ (ุจุงูุงุ ูพุงูุ ฺูพุ ุฑุงุณุช).
    # ุงุทููุงู ุงุฒ ุงูฺฉู ููู ุจุฑฺุณุจโูุงุ ุนููุงูโูุง ู ุฑุงูููุงูุง ุฏุฑ ูุญุฏูุฏู ุดฺฉู ุฌุง ุจฺฏุฑูุฏ.

    path = os.path.join(output_dir, "metrics_comparison.png")
    plt.savefig(path)
    plt.close()
    print(f"๐ผ Metrics comparison chart saved: {path}")

def plot_roc_curves(models, X_test, y_test, output_dir="outputs"):
    """ููุญู ROC ูููู ูุฏูโูุง ุฑุง ุฏุฑ ฺฉ ุดฺฉู ุฑุณู ู ุฐุฎุฑู ูโฺฉูุฏ."""
    os.makedirs(output_dir, exist_ok=True)
    plt.figure(figsize=(10, 6))
    for name, model in models.items():
        if hasattr(model, "predict_proba"):
        #ุชุงุจุน hasattr() ูฺฏู ุขุง ฺฉ ุดุก ุฏุงุฑุง ฺฉ ุตูุช (attribute) ุง ูุชุฏ (method) ุฎุงุต ุงุณุช ุง ุฎุฑ ุงฺฏุฑ ุจูุฏ True ุฑุง ุจุฑูฺฏุฑุฏุงูุฏ
            y_score = model.predict_proba(X_test)[:, 1]
            #ุฎุฑูุฌ ุฏู ุจุนุฏ ุงุณุช ุจูุงุจุฑุงู [:, 1] ุฑุง ูุฑุงุฑ ุฏุงุฏู
            #ฺูู ุฏู ฺฉูุงุณ ุฏุงุดุชู ู ุงฺฏุฑ ฺฉูุงุณูุง ุจุดุชุฑ ุฏุงุดุชู ุญุช ุฎุฑูุฌ ฺูุฏ ุจุนุฏ ูุดุฏ

        elif hasattr(model, "decision_function"):
            y_score = model.decision_function(X_test)
            #ุฏุฑ ุงูุฌุง ูุงุฒ ุจู ุงูุชุฎุงุจ ุณุชูู ุฎุงุต ูุณุชุ ุฒุฑุง ุฎุฑูุฌ ฺฉ ุจุนุฏ ุงุณุช ุจูุงุจุฑุงู ฺฉ ุณุชูู ูุฌูุฏ ุฏุงุฑุฏ

        else:
            print(f"โ๏ธ {name} cannot produce probability scores.")
            continue
        fpr, tpr, _ = roc_curve(y_test, y_score)
        # ุงู ุชุงุจุน ุณู ุฎุฑูุฌ ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ:
        #     fpr: ูุฑุฎ ูุซุจุช ฺฉุงุฐุจ (False Positive Rate)
        #     tpr: ูุฑุฎ ูุซุจุช ูุงูุน (True Positive Rate)
        #     thresholds: ุขุณุชุงููโูุง (thresholds) ฺฉู ุฏุฑ ุขูโูุง FPR ู TPR ูุญุงุณุจู ุดุฏูโุงูุฏ.
        # ูุชุบุฑ ุขูุฏุฑูุงู (_) ุฏุฑ ูพุงุชูู ฺฉ ูุฑุงุฑุฏุงุฏ (convention) ุงุณุช ฺฉู ุจู ูุนูุง "ุงู ูุชุบุฑ ุฑุง ูุงุฏุฏู ุจฺฏุฑ" ุง "ูู ุจู ุงู ููุฏุงุฑ ูุงุฒ ูุฏุงุฑู" ุงุณุช.

        roc_auc = auc(fpr, tpr)
        # ุนุจุงุฑุช auc ูุณุงุญุช ุฒุฑ ูุฑ ูููุฏุงุฑ ุฑุง ูุญุงุณุจู ูฺฉูุฏ

        # ููุญูโูุง ROC ููู ูุฏูโูุง (ฺูู ุฏุงุฎู ุญููู for ุงุณุช) ุฑู ฺฉ ูููุฏุงุฑ ูุงุญุฏ ุฑุณู ูโุดููุฏ
        plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.2f})")

    # ุฑุณู ุฎุท ฺู  ุฑู ูููุฏุงุฑ ROC  ฺฉู ุงุฒ ููุทู (0,0) ุจู (1,1) ูโุฑูุฏ.
    # ฺฉู ูพุณโุฒููู ุจุฑุง ููุงุณู ุนููฺฉุฑุฏ ุงุณุช
    plt.plot([0, 1], [0, 1], "k--")
    # ุนุจุงุฑุช [0, 1] ููุธูุฑ ูุฎุชุตุงุช ุงุณุช
    # ุนุจุงุฑุช  k-- ุนู ูุดฺฉ ุจุง ุงุณุชุงู __ ุงุณุช
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves")
    plt.legend(loc="lower right")
    plt.tight_layout()
    path = os.path.join(output_dir, "roc_curves.png")
    plt.savefig(path)
    plt.close()
    print(f"โ ROC curves saved: {path}")

