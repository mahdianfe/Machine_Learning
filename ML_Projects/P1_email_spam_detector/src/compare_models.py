# src/compare_models.py
# ูุงู ุงุฑุฒุงุจ ุง ุงุณุช ุชุง ุณู ูุฏู Naive Bayes ู  SVM ู Logistic Regression ุฑุง ุจุง ูู ููุงุณู ฺฉูุฏ


import os
import joblib
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#  ุฑุณู ูููุฏุงุฑ ูููโุง ุจุฑุง ููุงุณู ูุฏูโูุง
def plot_model_comparison(results, save_path):
    labels = ["Accuracy", "Precision", "Recall", "F1-Score"]
    model_names = [r[0] for r in results]
    #ุงู ฺฉ ูุณุชโุณุงุฒ (list comprehension) ุงุณุช
    #ุจุฑุง ูุฑ ุนูุตุฑ rุ ุงููู ุขุชู ุขู (ุจุง ุงูุฏุณ 0) ุงูุชุฎุงุจ ูโุดูุฏ.
    #ุฏุฑ ููุงุชุ ฺฉ ูุณุช ุฌุฏุฏ ุจู ูุงู model_names ุงุฌุงุฏ ูโุดูุฏ ฺฉู ุดุงูู ูุงู ุชูุงู ูุฏูโูุง ููุฌูุฏ ุฏุฑ results ุงุณุช.

    scores = [r[1:] for r in results]
    # ฺฉ ุจุฑุด (slice) ุงุฒ ุขู ุงุฌุงุฏ ูโุดูุฏ ฺฉู ุดุงูู ุชูุงู ุขุชูโูุง ุงุฒ ุงูุฏุณ 1 ุจู ุจุนุฏ ุงุณุช.
    #ุฏุฑ ููุงุช ฺฉ ูุณุช ุฌุฏุฏ ุจู ูุงู scores ุงุฌุงุฏ ูโุดูุฏ ฺฉู ุฏุฑ ุขู ูุฑ ุนูุตุฑุ ูุณุช ุงุฒ ููุฑุงุช ุงุฑุฒุงุจ ุจุฑุง ฺฉ ูุฏู ุฎุงุต ุงุณุช.
    scores = np.array(scores)
    # ุจุฑุง ุชุจุฏู ูุณุช scores ุจู ฺฉ ุขุฑุงู NumPy ุงุณุชูุงุฏู ูโุดูุฏ.
    # ุขุฑุงูโูุง NumPy ุจุฑุง ุงูุฌุงู ูุญุงุณุจุงุช ุนุฏุฏ ฺฉุงุฑุขูุฏุชุฑ ูุณุชูุฏ ู ุจุฑุง ุฑุณู ูููุฏุงุฑ ุจุง matplotlib ููุงุณุจโุชุฑูุฏ.

    x = np.arange(len(labels))
    #ุจุง ุงุณุชูุงุฏู ุงุฒ np.arange ฺฉ ุขุฑุงู NumPy ุงุฒ ุงุนุฏุงุฏ ุตุญุญ ุงุฌุงุฏ ูโุดูุฏ.

    width = 0.25 # ุงู ููุฏุงุฑ ุจุฑุง ุชุนู ูพููุง ูููโูุง ูููุฏุงุฑ
    fig, ax = plt.subplots(figsize=(10, 6))
    for i in range(len(model_names)):
        ax.bar(x + i * width, scores[i], width, label=model_names[i])
        #x + i * width: ูููุนุช x ุจุฑุง ูููโูุง ูุฑุจูุท ุจู ูุฏู ูุนู (i).
        # ุจุง ุงุถุงูู ฺฉุฑุฏู i * widthุ ูููโูุง ูุฑุจูุท ุจู ูุฑ ูุฏู ฺฉู ุจู ุณูุช ุฑุงุณุช ุฌุงุจุฌุง ูโุดููุฏ ุชุง ุงุฒ ูู ุฌุฏุง ุจุงุดูุฏ.

    ax.set_ylabel('Score') #ุนููุงู ฺฉู ุจุฑุง ูุญูุฑ y ุงุณุช ฺฉู 'Score' ุฑุง ฺฏุฐุงุดุชู.
    ax.set_title('Model Comparison')
    ax.set_xticks(x + width)
    #ูฺฉุงู ูุฑุงุฑฺฏุฑ ูุดุงููโูุง ูุญูุฑ x ุชูุธู ูโุดูุฏ.
    # ุงุฒ x + width ุงุณุชูุงุฏู ูโุดูุฏ ุชุง ูุดุงููโูุง ุฏุฑ ูุณุท ฺฏุฑููโูุง ูููโูุง ูุฑุงุฑ ุจฺฏุฑูุฏ.

    ax.set_xticklabels(labels) #ุจุฑฺุณุจโูุง ูุฑุจูุท ุจู ูุดุงููโูุง ูุญูุฑ x ุจุง ุงุณุชูุงุฏู ุงุฒ ูุณุช labels ุชูุธู ูโุดู
    ax.legend() #ุฑุงูููุง ูููุฏุงุฑ
    ax.grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    #ฺฉ ุชุงุจุน ฺฉู ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ ูุงุตููโุจูุฏ ุจู ุนูุงุตุฑ ูููุฏุงุฑ (ูุงููุฏ ุนููุงูุ ุจุฑฺุณุจโูุง ู ุฑุงูููุง) ุฑุง ุชูุธู ูโฺฉูุฏ ุชุง ุงุฒ ูููพูุดุงู ุฌููฺฏุฑ ุดูุฏ.

    plt.savefig(save_path)
    #ูููุฏุงุฑ ุชููุฏ ุดุฏู ุฏุฑ ูุงู ุจุง ูุณุฑ ฺฉู ุฏุฑ ูพุงุฑุงูุชุฑ save_path ูุดุฎุต ุดุฏู ุงุณุชุ ุฐุฎุฑู ูโุดูุฏ.

    plt.close()
    #ุดฺฉู (figure) ูุฑุจูุท ุจู ูููุฏุงุฑ ุจุณุชู ูโุดูุฏ ุชุง ููุงุจุน ุณุณุชู ุขุฒุงุฏ ุดููุฏ.

    print(f"โ ูููุฏุงุฑ ุฐุฎุฑู ุดุฏ: {save_path}")


#  ููุงุณู ูุฏูโูุง + ุชููุฏ ุฎุฑูุฌ ูุชู ู ุชุตูุฑ
def compare_models(X_test, y_test, models_dir):
    model_files = {
        "SVM": "svm_classifier.pkl",
        "Naive Bayes": "nb_classifier.pkl",
        "Logistic Regression": "lr_classifier.pkl"
    }

    results = []

    for model_name, filename in model_files.items():
        try:
            model_path = os.path.join(models_dir, filename)
            model = joblib.load(model_path)
            y_pred = model.predict(X_test)

            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred)
            rec = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)

            results.append((model_name, acc, prec, rec, f1))

            print(f"\n๐ {model_name} Scores:")
            print(f"Accuracy: {acc:.4f}")
            print(f"Precision: {prec:.4f}")
            print(f"Recall: {rec:.4f}")
            print(f"F1-Score: {f1:.4f}")

        except Exception as e:
            print(f"โ๏ธ ุฎุทุง ุฏุฑ ุจุงุฑฺฏุฐุงุฑ ุง ุงุฑุฒุงุจ {model_name}: {e}")

    #  ูุณุฑ ุฎุฑูุฌโูุง: ุฏุงุฎู src/evaluation_outputs
    base_dir = os.path.dirname(os.path.abspath(__file__))
    outputs_dir = os.path.join(base_dir, 'evaluation_outputs')
    os.makedirs(outputs_dir, exist_ok=True)

    #  ฺฏุฒุงุฑุด ูุชู
    report_path = os.path.join(outputs_dir, 'compare_models_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Model Comparison:\n")
        f.write(f"{'Model':<20}{'Accuracy':<10}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}\n")
        for model_name, acc, prec, rec, f1 in results:
            line = f"{model_name:<20}{acc:<10.2f}{prec:<10.2f}{rec:<10.2f}{f1:<10.2f}\n"
            f.write(line)
            print(line, end='')

    print(f"\nโ ฺฏุฒุงุฑุด ูุชู ุฐุฎุฑู ุดุฏ: {os.path.abspath(report_path)}")

    # ๐ผ๏ธ ุฐุฎุฑู ูููุฏุงุฑ
    chart_path = os.path.join(outputs_dir, 'compare_models_chart.png')
    plot_model_comparison(results, chart_path)

#  ุชุงุจุน main ููุท ุจุฑุง ุชุณุช (ุงุฎุชุงุฑ)
def main():
    import pandas as pd
    from sklearn.model_selection import train_test_split

    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'processed_data.csv')
    df = pd.read_csv(data_path)

    X = df.drop('label', axis=1)
    y = df['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    models_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    compare_models(X_test, y_test, models_dir)

if __name__ == "__main__":
    main()

"""
ุจุฑุง ุฎุฑูุฌ ุงู ูุงู ุจุงุฏ ุงุฒ ูุงู evaluate_models.py ุฎุฑูุฌ ฺฏุฑูุชู ุดูุฏ  
"""